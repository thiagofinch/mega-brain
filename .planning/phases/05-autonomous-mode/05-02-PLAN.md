---
phase: 05-autonomous-mode
plan: 02
type: execute
wave: 2
depends_on:
  - 05-01
files_modified:
  - core/intelligence/autonomous_processor.py
  - .claude/mission-control/AUTONOMOUS-MONITOR.json
autonomous: true
requirements:
  - AUTO-04
  - AUTO-05

must_haves:
  truths:
    - "Monitoring JSON file is updated in real-time during processing"
    - "Checkpoint is saved every N files (configurable)"
    - "Processing can resume from last checkpoint after crash"
  artifacts:
    - path: "core/intelligence/autonomous_processor.py"
      provides: "Extended with monitoring and checkpoint capabilities"
      exports:
        - AutonomousProcessor
        - MonitoringStatus
    - path: ".claude/mission-control/AUTONOMOUS-MONITOR.json"
      provides: "Real-time monitoring status"
      contains: "current_file"
  key_links:
    - from: "core/intelligence/autonomous_processor.py"
      to: ".claude/mission-control/AUTONOMOUS-MONITOR.json"
      via: "JSON write"
      pattern: "AUTONOMOUS-MONITOR"
    - from: "core/intelligence/autonomous_processor.py"
      to: ".claude/mission-control/AUTONOMOUS-CHECKPOINT.json"
      via: "Checkpoint save"
      pattern: "AUTONOMOUS-CHECKPOINT"
---

<objective>
Add real-time monitoring and checkpoint capabilities to the autonomous processor.

Purpose: Enable status visibility and crash recovery for long-running autonomous processing
Output: Extended `autonomous_processor.py` with monitoring JSON and checkpoint system
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-autonomous-mode/05-01-SUMMARY.md
@core/intelligence/autonomous_processor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement real-time monitoring status</name>
  <files>core/intelligence/autonomous_processor.py</files>
  <action>
Add monitoring capabilities to autonomous_processor.py:

```python
# Constants
MONITOR_PATH = PROJECT_DIR / '.claude' / 'mission-control' / 'AUTONOMOUS-MONITOR.json'

@dataclass
class MonitoringStatus:
    """Real-time status for external monitoring."""
    status: str  # idle, running, stopped, completed
    current_file: Optional[str]
    files_in_queue: int
    files_processed: int
    files_failed: int
    started_at: Optional[str]
    last_updated: str
    current_file_started: Optional[str]
    estimated_remaining_files: int
    avg_file_duration_seconds: float
    error_rate_percent: float

# Add to AutonomousProcessor class:

def _update_monitor(self) -> None:
    """Update monitoring JSON file with current status."""
    total_completed = self.state.files_processed + self.state.files_failed
    error_rate = (self.state.files_failed / total_completed * 100) if total_completed > 0 else 0

    # Calculate average duration from completed files
    avg_duration = self._calculate_avg_duration()

    status = MonitoringStatus(
        status=self.state.status,
        current_file=self.state.current_file,
        files_in_queue=self.queue.size(),
        files_processed=self.state.files_processed,
        files_failed=self.state.files_failed,
        started_at=self.state.started_at,
        last_updated=datetime.utcnow().isoformat(),
        current_file_started=self._current_file_started,
        estimated_remaining_files=self.queue.size(),
        avg_file_duration_seconds=avg_duration,
        error_rate_percent=round(error_rate, 1)
    )

    MONITOR_PATH.parent.mkdir(parents=True, exist_ok=True)
    with open(MONITOR_PATH, 'w') as f:
        json.dump(asdict(status), f, indent=2)

def _calculate_avg_duration(self) -> float:
    """Calculate average file processing duration."""
    if not hasattr(self, '_durations') or not self._durations:
        return 0.0
    return sum(self._durations) / len(self._durations)
```

Modify the run() loop to call `_update_monitor()`:
- Before starting each file
- After completing each file
- On stop signal
- On completion

Add CLI command:
```python
elif command == 'monitor':
    # Read and pretty-print AUTONOMOUS-MONITOR.json
    if MONITOR_PATH.exists():
        with open(MONITOR_PATH) as f:
            data = json.load(f)
        print("[JARVIS] Autonomous Processor Monitor")
        print(f"  Status: {data['status']}")
        print(f"  Current File: {data.get('current_file', 'None')}")
        print(f"  Queue: {data['files_in_queue']} files")
        print(f"  Processed: {data['files_processed']}")
        print(f"  Failed: {data['files_failed']}")
        print(f"  Error Rate: {data['error_rate_percent']}%")
        print(f"  Avg Duration: {data['avg_file_duration_seconds']:.1f}s")
        print(f"  Last Updated: {data['last_updated']}")
    else:
        print("[JARVIS] No monitoring data. Processor not running.")
```
  </action>
  <verify>
```bash
python3 -c "
from core.intelligence.autonomous_processor import AutonomousProcessor, MONITOR_PATH
import json

# Create processor and trigger monitor update
proc = AutonomousProcessor('wf-ingest')
proc.state.status = 'running'
proc.state.files_processed = 5
proc.state.files_failed = 1
proc._update_monitor()

# Verify monitor file
assert MONITOR_PATH.exists(), 'Monitor file should exist'
with open(MONITOR_PATH) as f:
    data = json.load(f)
assert data['status'] == 'running'
assert data['files_processed'] == 5
assert 'error_rate_percent' in data
print('Monitoring: PASSED')
"
```
  </verify>
  <done>AUTONOMOUS-MONITOR.json is updated during processing with current_file, queue size, error rate</done>
</task>

<task type="auto">
  <name>Task 2: Implement checkpoint system for crash recovery</name>
  <files>core/intelligence/autonomous_processor.py</files>
  <action>
Add checkpoint capabilities to autonomous_processor.py:

```python
# Constants
CHECKPOINT_PATH = PROJECT_DIR / '.claude' / 'mission-control' / 'AUTONOMOUS-CHECKPOINT.json'
DEFAULT_CHECKPOINT_INTERVAL = 5  # Save checkpoint every N files

@dataclass
class Checkpoint:
    """Checkpoint for crash recovery."""
    version: str = "1.0.0"
    created_at: str = ""
    queue_snapshot: List[Dict[str, Any]] = field(default_factory=list)
    processor_state: Dict[str, Any] = field(default_factory=dict)
    files_processed_since_start: int = 0
    last_completed_file: Optional[str] = None

# Add to AutonomousProcessor class:

def __init__(self, workflow_id: str = "wf-pipeline-full", checkpoint_interval: int = DEFAULT_CHECKPOINT_INTERVAL):
    # ... existing init ...
    self.checkpoint_interval = checkpoint_interval
    self._files_since_checkpoint = 0
    self._durations: List[float] = []
    self._current_file_started: Optional[str] = None

def _save_checkpoint(self) -> None:
    """Save checkpoint for crash recovery."""
    checkpoint = Checkpoint(
        version="1.0.0",
        created_at=datetime.utcnow().isoformat(),
        queue_snapshot=[asdict(item) for item in self.queue.items],
        processor_state=asdict(self.state),
        files_processed_since_start=self.state.files_processed,
        last_completed_file=self._last_completed_file if hasattr(self, '_last_completed_file') else None
    )

    CHECKPOINT_PATH.parent.mkdir(parents=True, exist_ok=True)
    with open(CHECKPOINT_PATH, 'w') as f:
        json.dump(asdict(checkpoint), f, indent=2)

    self._log_event({
        'event': 'checkpoint_saved',
        'files_processed': self.state.files_processed,
        'queue_size': self.queue.size()
    })

def _should_checkpoint(self) -> bool:
    """Check if checkpoint should be saved (every N files)."""
    return self._files_since_checkpoint >= self.checkpoint_interval

def _restore_from_checkpoint(self) -> bool:
    """
    Restore state from checkpoint if available.

    Returns:
        True if restored, False if no checkpoint
    """
    if not CHECKPOINT_PATH.exists():
        return False

    try:
        with open(CHECKPOINT_PATH) as f:
            data = json.load(f)

        # Restore queue
        self.queue.items = [QueueItem(**item) for item in data['queue_snapshot']]
        self.queue._save()

        # Restore processor state
        state_data = data['processor_state']
        self.state = ProcessorState(**state_data)

        self._log_event({
            'event': 'checkpoint_restored',
            'from_checkpoint': data['created_at'],
            'queue_size': len(self.queue.items)
        })

        return True
    except Exception as e:
        self._log_event({
            'event': 'checkpoint_restore_failed',
            'error': str(e)
        })
        return False

def resume(self) -> Dict[str, Any]:
    """
    Resume processing from last checkpoint.

    Returns:
        Result of run() or error dict
    """
    restored = self._restore_from_checkpoint()
    if not restored:
        return {'success': False, 'error': 'No checkpoint found'}

    return self.run()
```

Modify run() loop to save checkpoints:
```python
def run(self, timeout_seconds: Optional[int] = None) -> Dict[str, Any]:
    # ... existing setup ...

    while not self._should_stop() and not self.queue.is_empty():
        item = self.queue.pop()
        if not item:
            break

        result = self._process_file(item)

        if result.success:
            self.state.files_processed += 1
            self._last_completed_file = result.file_path
        else:
            self.state.files_failed += 1
            if self._should_retry(item):
                self._requeue_with_backoff(item)

        # Checkpoint every N files
        self._files_since_checkpoint += 1
        if self._should_checkpoint():
            self._save_checkpoint()
            self._files_since_checkpoint = 0

        self._update_monitor()

    # ... rest of method ...
```

Add CLI commands:
```python
elif command == 'checkpoint':
    subcommand = sys.argv[2] if len(sys.argv) > 2 else 'show'
    if subcommand == 'show':
        if CHECKPOINT_PATH.exists():
            with open(CHECKPOINT_PATH) as f:
                data = json.load(f)
            print("[JARVIS] Checkpoint Info")
            print(f"  Created: {data['created_at']}")
            print(f"  Files Processed: {data['files_processed_since_start']}")
            print(f"  Queue Snapshot: {len(data['queue_snapshot'])} items")
            print(f"  Last Completed: {data.get('last_completed_file', 'None')}")
        else:
            print("[JARVIS] No checkpoint exists.")
    elif subcommand == 'clear':
        if CHECKPOINT_PATH.exists():
            CHECKPOINT_PATH.unlink()
        print("[JARVIS] Checkpoint cleared.")

elif command == 'resume':
    proc = AutonomousProcessor()
    result = proc.resume()
    if result.get('success'):
        print(f"[JARVIS] Resumed and completed. Processed: {result.get('processed', 0)}")
    else:
        print(f"[JARVIS] Resume failed: {result.get('error')}")
```
  </action>
  <verify>
```bash
python3 -c "
from core.intelligence.autonomous_processor import AutonomousProcessor, CHECKPOINT_PATH
import json

# Create processor
proc = AutonomousProcessor('wf-ingest', checkpoint_interval=2)
proc.queue.clear()

# Add some items and save checkpoint
proc.queue.add('/test/file1.txt')
proc.queue.add('/test/file2.txt')
proc.state.files_processed = 10
proc._save_checkpoint()

# Verify checkpoint file
assert CHECKPOINT_PATH.exists(), 'Checkpoint should exist'
with open(CHECKPOINT_PATH) as f:
    data = json.load(f)
assert data['files_processed_since_start'] == 10
assert len(data['queue_snapshot']) == 2

# Test restore
proc2 = AutonomousProcessor('wf-ingest')
proc2.queue.clear()
proc2.state.files_processed = 0
restored = proc2._restore_from_checkpoint()
assert restored, 'Should restore from checkpoint'
assert proc2.state.files_processed == 10

print('Checkpoint: PASSED')
"
```
  </verify>
  <done>Checkpoint saved every N files, resume command restores from checkpoint</done>
</task>

<task type="auto">
  <name>Task 3: Update CLI help and module exports</name>
  <files>core/intelligence/autonomous_processor.py, core/intelligence/__init__.py</files>
  <action>
Update print_usage() to include all commands:

```python
def print_usage():
    print("""
Autonomous Processor - Mega Brain Pipeline
===========================================

Usage:
    python3 autonomous_processor.py <command> [args]

Queue Commands:
    queue add <file> [priority]   Add file to queue (priority: higher = first)
    queue list                    List pending files
    queue clear                   Clear queue
    queue size                    Show queue size

Processing Commands:
    run [--timeout=300] [--checkpoint=5]   Start processing
    stop                                   Send stop signal
    resume                                 Resume from checkpoint

Monitoring Commands:
    status                        Show processor status
    monitor                       Show real-time monitoring
    checkpoint show               Show checkpoint info
    checkpoint clear              Clear checkpoint

Recovery Commands:
    retry-failed                  Re-queue all failed files

Examples:
    python3 autonomous_processor.py queue add inbox/file.txt 1
    python3 autonomous_processor.py run --timeout=600 --checkpoint=10
    python3 autonomous_processor.py monitor
    python3 autonomous_processor.py resume
    """)
```

Update main() to handle --timeout and --checkpoint flags:
```python
elif command == 'run':
    timeout = DEFAULT_TIMEOUT_SECONDS
    checkpoint_interval = DEFAULT_CHECKPOINT_INTERVAL

    for arg in sys.argv[2:]:
        if arg.startswith('--timeout='):
            timeout = int(arg.split('=')[1])
        elif arg.startswith('--checkpoint='):
            checkpoint_interval = int(arg.split('=')[1])

    proc = AutonomousProcessor(checkpoint_interval=checkpoint_interval)
    proc.timeout_seconds = timeout
    result = proc.run()
    print(f"[JARVIS] Processing complete.")
    print(f"  Processed: {result.get('processed', 0)}")
    print(f"  Failed: {result.get('failed', 0)}")
    print(f"  Stopped by: {result.get('stopped_by', 'queue_empty')}")
```

Update __all__ in autonomous_processor.py:
```python
__all__ = [
    'AutonomousProcessor',
    'FileQueue',
    'QueueItem',
    'ProcessingResult',
    'ProcessorState',
    'MonitoringStatus',
    'Checkpoint',
    'DEFAULT_TIMEOUT_SECONDS',
    'DEFAULT_CHECKPOINT_INTERVAL',
    'MAX_RETRIES',
]
```

Update core/intelligence/__init__.py to include new exports:
```python
from .autonomous_processor import (
    AutonomousProcessor,
    FileQueue,
    QueueItem,
    ProcessingResult,
    ProcessorState,
    MonitoringStatus,
    Checkpoint,
)
```
  </action>
  <verify>
```bash
python3 core/intelligence/autonomous_processor.py --help 2>&1 | head -20 && \
python3 core/intelligence/autonomous_processor.py checkpoint show && \
python3 -c "from core.intelligence import AutonomousProcessor, MonitoringStatus, Checkpoint; print('All exports: OK')"
```
  </verify>
  <done>All CLI commands documented, --timeout/--checkpoint flags work, all types exported</done>
</task>

</tasks>

<verification>
All AUTO-04, AUTO-05 requirements implemented:
1. `monitor` command shows real-time status from AUTONOMOUS-MONITOR.json
2. `checkpoint show` displays checkpoint info
3. `resume` restores from checkpoint and continues processing
4. Checkpoint saved every N files (configurable via --checkpoint=N)
5. All new types exported from core.intelligence
</verification>

<success_criteria>
- AUTONOMOUS-MONITOR.json updated in real-time with current_file, queue size, error rate
- Checkpoint saved every N files (default 5, configurable)
- `resume` command restores queue and state from checkpoint
- CLI commands: monitor, checkpoint show/clear, resume
- MonitoringStatus and Checkpoint types exported from module
</success_criteria>

<output>
After completion, create `.planning/phases/05-autonomous-mode/05-02-SUMMARY.md`
</output>
