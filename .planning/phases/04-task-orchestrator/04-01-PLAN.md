---
phase: 04-task-orchestrator
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - core/intelligence/task_orchestrator.py
  - core/intelligence/__init__.py
autonomous: true
requirements: [ORCH-01, ORCH-02]

must_haves:
  truths:
    - "Orchestrator can parse and load workflow YAML files"
    - "Orchestrator can identify and resolve task references"
    - "Orchestrator executes tasks in the order defined by workflow phases"
    - "Orchestrator tracks execution state between tasks"
  artifacts:
    - path: "core/intelligence/task_orchestrator.py"
      provides: "Task orchestrator implementation"
      min_lines: 300
      exports: ["TaskOrchestrator", "load_workflow", "execute_workflow"]
  key_links:
    - from: "core/intelligence/task_orchestrator.py"
      to: "core/workflows/*.yaml"
      via: "YAML file loading"
      pattern: "yaml\\.safe_load"
    - from: "core/intelligence/task_orchestrator.py"
      to: "core/tasks/*.md"
      via: "task resolution"
      pattern: "Path.*tasks"
---

<objective>
Create the core task orchestrator that reads workflow YAML files from `core/workflows/` and executes tasks sequentially, tracking state between tasks.

Purpose: Enable programmatic execution of the Mega Brain pipeline workflows, replacing manual step-by-step orchestration with automated task sequencing.

Output: `core/intelligence/task_orchestrator.py` - A Python module implementing workflow parsing, task resolution, and sequential execution.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-checkpoint-hooks/03-01-SUMMARY.md

<interfaces>
<!-- Existing workflow YAML structure (from core/workflows/wf-pipeline-full.yaml) -->
```yaml
workflow:
  id: wf-pipeline-full
  name: "Full Pipeline Processing"
  phases:
    - id: phase_1
      name: "FOUNDATION"
      steps:
        - execute:
            task: "tasks/normalize-entities.md"
            inputs:
              content: "${files}"
      checkpoint:
        id: CP_FOUNDATION
        blocking: true
        validation:
          - "entities_normalized"
  transitions:
    - from: phase_1
      to: phase_2
      condition: "checkpoint_passed"
```

<!-- Existing task structure (from core/tasks/process-batch.md) -->
```markdown
# Task Anatomy
| Field | Value |
| task_name | Process Content Batch |
| execution_type | Agent |
| responsible | @jarvis |
| input | batch files, source config |
| output | BATCH-XXX.md, insights, cascading |

## Inputs
| Input | Type | Required | Description |

## Outputs
| Output | Type | Location | Description |

## Execution
### Phase N: [Name]
**Quality Gate:** QG-XXX
1. Step...
2. Step...
```

<!-- Python project conventions from core/intelligence/ -->
- Use pathlib.Path for all file operations
- Type hints required
- Comprehensive docstrings
- Follow existing patterns in validate_json_integrity.py, role_detector.py
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create task_orchestrator.py core module</name>
  <files>core/intelligence/task_orchestrator.py</files>
  <action>
Create `core/intelligence/task_orchestrator.py` implementing:

1. **Configuration and Constants (top of file):**
   - PROJECT_DIR from env CLAUDE_PROJECT_DIR or '.'
   - WORKFLOW_DIR = PROJECT_DIR / 'core' / 'workflows'
   - TASK_DIR = PROJECT_DIR / 'core' / 'tasks'
   - STATE_PATH = PROJECT_DIR / '.claude' / 'mission-control' / 'ORCHESTRATOR-STATE.json'
   - LOG_PATH = PROJECT_DIR / 'logs' / 'orchestrator-execution.jsonl'

2. **Data Classes:**
   - `TaskDefinition`: task_id, name, execution_type, responsible, inputs, outputs
   - `WorkflowPhase`: id, name, description, steps, checkpoint, order
   - `WorkflowDefinition`: id, name, description, phases, transitions, inputs, outputs
   - `ExecutionState`: workflow_id, current_phase, current_step, status, history, started_at, completed_at

3. **YAML Loading Functions:**
   - `load_workflow(workflow_path: Path) -> WorkflowDefinition`: Parse YAML, validate structure, return typed object
   - `list_workflows() -> List[Path]`: List all .yaml files in WORKFLOW_DIR
   - `resolve_workflow(workflow_id: str) -> Optional[Path]`: Find workflow file by ID

4. **Task Resolution Functions:**
   - `load_task_definition(task_path: str) -> TaskDefinition`: Parse task markdown, extract anatomy table, inputs, outputs
   - `resolve_task(task_ref: str) -> Path`: Convert task reference (e.g., "tasks/process-batch.md") to full path

5. **State Management:**
   - `create_default_state(workflow_id: str) -> ExecutionState`
   - `load_state() -> Optional[ExecutionState]`
   - `save_state(state: ExecutionState) -> None`
   - `log_execution(event: Dict) -> None`: Append to JSONL log

6. **Core Orchestrator Class - TaskOrchestrator:**
   ```python
   class TaskOrchestrator:
       def __init__(self, workflow_id: str):
           self.workflow = load_workflow(resolve_workflow(workflow_id))
           self.state = load_state() or create_default_state(workflow_id)
           self.task_cache: Dict[str, TaskDefinition] = {}

       def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
           """Execute workflow phases sequentially."""
           for phase in self.workflow.phases:
               result = self._execute_phase(phase, inputs)
               if not result['success']:
                   return result
               inputs.update(result.get('outputs', {}))
           return {'success': True, 'outputs': inputs}

       def _execute_phase(self, phase: WorkflowPhase, inputs: Dict) -> Dict:
           """Execute all steps in a phase."""
           self.state.current_phase = phase.id
           save_state(self.state)

           for step in phase.steps:
               result = self._execute_step(step, inputs)
               if not result['success']:
                   return result
           return {'success': True}

       def _execute_step(self, step: Dict, inputs: Dict) -> Dict:
           """Execute a single step (task or command)."""
           if 'execute' in step:
               exec_config = step['execute']
               if 'task' in exec_config:
                   task_def = self._resolve_and_cache_task(exec_config['task'])
                   # Log task execution (actual execution delegated to Claude)
                   return {'success': True, 'task': task_def, 'needs_execution': True}
           return {'success': True}

       def get_current_task(self) -> Optional[TaskDefinition]:
           """Return the current task to execute."""

       def mark_task_complete(self, task_id: str, outputs: Dict) -> None:
           """Mark a task as complete and record outputs."""
   ```

7. **Include comprehensive docstrings following project patterns.**

Use standard library only (json, yaml via PyYAML which is allowed per CLAUDE.md, pathlib, dataclasses, datetime, re).
  </action>
  <verify>
python3 -c "
import sys
sys.path.insert(0, 'core/intelligence')
from task_orchestrator import TaskOrchestrator, load_workflow, list_workflows
workflows = list_workflows()
assert len(workflows) >= 4, f'Expected 4+ workflows, got {len(workflows)}'
print('TaskOrchestrator importable:', 'TaskOrchestrator' in dir())
print('Workflows found:', len(workflows))
"
  </verify>
  <done>
- TaskOrchestrator class exists and is importable
- load_workflow can parse existing YAML files
- list_workflows returns 4+ workflow files
- State management functions exist
  </done>
</task>

<task type="auto">
  <name>Task 2: Update __init__.py and add integration tests</name>
  <files>core/intelligence/__init__.py</files>
  <action>
1. **Update `core/intelligence/__init__.py`:**
   Add task_orchestrator to the module exports:
   ```python
   from .task_orchestrator import (
       TaskOrchestrator,
       load_workflow,
       list_workflows,
       resolve_workflow,
       load_task_definition,
   )
   ```

2. **Verify workflow parsing works with all 4 existing workflows:**
   - wf-pipeline-full.yaml
   - wf-ingest.yaml
   - wf-conclave.yaml
   - wf-extract-dna.yaml

3. **Test task resolution for existing tasks:**
   - tasks/process-batch.md
   - tasks/extract-dna.md
   - tasks/detect-role.md
  </action>
  <verify>
python3 -c "
from core.intelligence import TaskOrchestrator, load_workflow, list_workflows
from pathlib import Path

# Test workflow loading
workflows = list_workflows()
for wf_path in workflows:
    wf = load_workflow(wf_path)
    assert wf.id is not None, f'Workflow {wf_path} missing id'
    print(f'Loaded: {wf.id} ({len(wf.phases)} phases)')

# Test orchestrator instantiation
orch = TaskOrchestrator('wf-ingest')
print(f'Orchestrator created for: {orch.workflow.id}')
print('All tests passed!')
"
  </verify>
  <done>
- All 4 workflows parse without error
- TaskOrchestrator can be instantiated with any workflow ID
- Module exports are correct in __init__.py
  </done>
</task>

</tasks>

<verification>
1. `python3 -c "from core.intelligence import TaskOrchestrator"` succeeds
2. All 4 workflows in core/workflows/ load correctly
3. State file structure is valid JSON
4. ORCHESTRATOR-STATE.json can be created/loaded
</verification>

<success_criteria>
- ORCH-01: Task orchestrator reads workflows YAML ✓
- ORCH-02: Task orchestrator executes tasks sequentially ✓ (execution framework ready)
- TaskOrchestrator class is importable and functional
- State persistence works (create/load/save)
</success_criteria>

<output>
After completion, create `.planning/phases/04-task-orchestrator/04-01-SUMMARY.md`
</output>
