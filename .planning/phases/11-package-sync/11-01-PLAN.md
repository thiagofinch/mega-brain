---
phase: 11-package-sync
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - core/intelligence/sync_package_files.py
  - package.json
autonomous: true
requirements:
  - SYNC-01
  - SYNC-03
must_haves:
  truths:
    - "Running sync_package_files.py outputs a JSON array of files entries derived from audit_layers.py L1 classifications"
    - "package.json files field matches the script output exactly"
    - "npm pack --dry-run includes only L1 content and no L2/L3/NEVER files"
    - "Running the audit confirms 0 DELETE items remain in the repository"
  artifacts:
    - path: "core/intelligence/sync_package_files.py"
      provides: "Script that reads L1 audit classifications and generates optimal package.json files array"
      contains: "from audit_layers import"
    - path: "package.json"
      provides: "Updated files field derived from audit, not hand-curated"
      contains: '"files"'
  key_links:
    - from: "core/intelligence/sync_package_files.py"
      to: "core/intelligence/audit_layers.py"
      via: "imports classify_path and scan_repository"
      pattern: "from audit_layers import"
    - from: "core/intelligence/sync_package_files.py"
      to: "package.json"
      via: "reads and writes files field"
      pattern: "package.json"
---

<objective>
Create a Python script that derives the package.json `files` field from the L1 audit classifications, then apply its output to package.json.

Purpose: Eliminate the hand-curated 91-entry files field. The audit is the single source of truth for what is L1 (publishable). The script bridges audit → package.json automatically.
Output: `sync_package_files.py` script + updated `package.json` with audit-derived files field.
</objective>

<execution_context>
@./.claude/agents/gsd-executor.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-audit-resolution/10-02-SUMMARY.md

<interfaces>
<!-- Key interfaces from audit_layers.py that the sync script will use -->

From core/intelligence/audit_layers.py:
```python
def classify_path(path: Path, repo_root: Path, is_file: bool) -> Tuple[str, str]:
    """Classify a path into a layer. Returns: (layer, reason)"""

def scan_repository(repo_root: Path, verbose: bool = False) -> Dict:
    """Scan entire repository and classify all items.
    Returns: {
        'summary': {'total_items': int, 'by_layer': {'L1': int, ...}},
        'classifications': [{'path': str, 'layer': str, 'type': str, 'reason': str}, ...],
        'delete_candidates': [...],
        'review_needed': [...]
    }
    """
```

Current package.json files field: 91 hand-curated entries (mix of directories like "bin/" and specific files like "inbox/.gitkeep").

Audit results (Phase 10 clean): 0 REVIEW, 0 DELETE, 853 L1 directories, 671 L1 files.

Key insight: Some directories are PURE L1 (bin/, .planning/, .cursor/, .windsurf/, .antigravity/) and can be listed as directory globs. Mixed directories (agents/, .claude/, knowledge/) need selective entries — either subdirectory globs or individual file paths.

Current package.json files format uses:
- Directory globs: `"bin/"`, `"core/"`
- Specific subdirs: `".claude/skills/code-review/"`
- Specific files: `"inbox/.gitkeep"`, `".claude/settings.json"`
</interfaces>

<analysis>
## Strategy for Optimal files Array

The script must produce the SMALLEST set of entries that exactly captures all L1 content:

1. **Pure L1 directories** (all files within are L1): Use directory glob `"dirname/"` — one entry covers many files
2. **Mixed directories** with L1 subdirs: Enumerate L1 subdirectories explicitly (e.g., `.claude/commands/`, `.claude/hooks/`)
3. **Individual L1 files** in non-L1 directories: List specific file paths (e.g., `inbox/.gitkeep`, `knowledge/dna/.gitkeep`)
4. **Root-level files**: List each one (e.g., `package.json` is implicit, but `.gitignore`, `CONTRIBUTING.md` need explicit listing)

The algorithm:
- For each L1 file, walk up to find the deepest directory where ALL descendants are L1
- If such a directory exists and isn't the repo root, add it as a directory glob
- If the file is standalone L1 in a non-L1 directory, add the specific file path
- Deduplicate: if a directory glob covers a file, don't add the file

CRITICAL: `package.json` itself is auto-included by npm — do NOT add it to files array.
CRITICAL: Root `README.md` is auto-included by npm — do NOT add it to files array.

## DELETE Candidates (SYNC-03)

Phase 10 already cleaned all DELETE items (4 stale macOS duplicates removed). The audit confirms 0 DELETE remaining. SYNC-03 is satisfied by verifying this holds — no action needed beyond confirmation.
</analysis>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create sync_package_files.py</name>
  <files>core/intelligence/sync_package_files.py</files>
  <action>
Create `core/intelligence/sync_package_files.py` — a script that:

1. Imports and runs `scan_repository()` from `audit_layers.py`
2. Extracts all L1-classified file paths
3. Computes the optimal `files` array using directory rollup
4. Can either `--print` the array (JSON to stdout) or `--apply` it to package.json

**Algorithm for directory rollup:**

```
For each L1 file path:
  1. Get all ancestor directories
  2. For each ancestor dir (deepest first):
     - Check if ALL files under this dir in the audit are L1
     - If yes: this dir is a "pure L1 directory", add "dir/" to files set
     - If no: continue to next ancestor
  3. If no pure ancestor found: add the specific file path

Deduplication:
  - Remove any specific file path if it's already covered by a directory entry
  - Remove any subdirectory if it's already covered by a parent directory entry
```

**Special cases to handle:**
- `package.json` and `README.md` are auto-included by npm — EXCLUDE from output
- `.gitignore` is explicitly EXCLUDED by npm unless in `files` — INCLUDE it
- `.npmignore` is used by npm but not packed — EXCLUDE from output
- `package-lock.json` should NOT be in files (npm excludes it from pack regardless)
- Root files like `CONTRIBUTING.md`, `.gitattributes`, `.gitleaks.toml`, `QUICK-START.md`, `.claudeignore` — INCLUDE
- `.planning/` directory — EXCLUDE from npm package (development-only, not for consumers)
- `docs/audit/` — EXCLUDE from npm package (generated reports, not for consumers)

**CLI interface:**

```
python3 core/intelligence/sync_package_files.py --print     # JSON array to stdout
python3 core/intelligence/sync_package_files.py --apply      # Update package.json in-place
python3 core/intelligence/sync_package_files.py --diff       # Show what would change
```

**Implementation details:**

```python
#!/usr/bin/env python3
"""
Sync package.json files field with L1 audit classifications.

Reads audit_layers.py classifications, computes optimal files array,
and can apply it to package.json.

Usage:
    python3 core/intelligence/sync_package_files.py --print   # JSON to stdout
    python3 core/intelligence/sync_package_files.py --apply   # Update package.json
    python3 core/intelligence/sync_package_files.py --diff    # Show changes
"""
import sys
import json
import argparse
from pathlib import Path
from collections import defaultdict

sys.path.insert(0, str(Path(__file__).parent))
from audit_layers import scan_repository

# Files auto-included by npm (do NOT add to files array)
NPM_AUTO_INCLUDED = {
    'package.json',
    'README.md',
    'README',
    'CHANGELOG.md',
    'CHANGELOG',
    'LICENSE',
    'LICENSE.md',
    'LICENCE',
    'LICENCE.md',
}

# L1 paths to EXCLUDE from npm package
# (development-only content that shouldn't ship to consumers)
EXCLUDE_FROM_PACKAGE = {
    '.planning/',       # GSD planning system (dev-only)
    'docs/audit/',      # Generated audit reports (dev-only)
    '.npmignore',       # npm uses it but doesn't pack it
    'package-lock.json', # npm excludes this from pack
}
```

The script should:
1. Run `scan_repository()` to get all classifications
2. Filter to L1 files only
3. Remove auto-included files (package.json, README.md) and EXCLUDE_FROM_PACKAGE entries
4. Group files by directory path
5. For each directory, check if ALL files in that directory (across ALL layers) are L1 — if so, use the directory glob
6. For mixed directories, recurse into subdirectories and apply the same logic
7. Remaining individual files get listed as specific paths
8. Sort the output alphabetically
9. Print/apply/diff as requested

When `--apply` is used:
- Read package.json
- Replace the `files` array with the computed one
- Write back with 2-space indent
- Print count of entries and diff summary

When `--diff` is used:
- Read current package.json files array
- Compare with computed array
- Print added/removed entries

**IMPORTANT:** The script must be IDEMPOTENT — running it twice produces the same result. And it must be DETERMINISTIC — same repo state always produces same output.
  </action>
  <verify>
    <automated>python3 core/intelligence/sync_package_files.py --print 2>/dev/null | python3 -c "import json,sys; d=json.load(sys.stdin); print(f'{len(d)} entries'); assert len(d) > 30, 'Too few entries'; assert 'package.json' not in d, 'package.json should not be in files'; assert 'README.md' not in d, 'README.md should not be in files'; assert any('bin/' in e for e in d), 'bin/ should be present'; print('PASS')"</automated>
  </verify>
  <done>sync_package_files.py produces a valid JSON array of files entries derived from L1 audit, excludes npm auto-included files and dev-only content, and the output is deterministic and idempotent</done>
</task>

<task type="auto">
  <name>Task 2: Apply sync output to package.json and verify</name>
  <files>package.json</files>
  <action>
Run the sync script to update package.json:

```bash
python3 core/intelligence/sync_package_files.py --apply
```

Then verify the result:

1. Run `npm pack --dry-run --json` and capture the file list
2. For each file in the pack output, verify it's classified as L1 by the audit
3. Confirm no L2/L3/NEVER files appear in the pack output
4. Confirm 0 DELETE items in the audit (SYNC-03 verification)

**Verification script (run inline):**

```bash
# Step 1: Apply
python3 core/intelligence/sync_package_files.py --apply

# Step 2: Dry-run pack
npm pack --dry-run 2>&1 | head -50

# Step 3: Verify no L2/L3/NEVER in pack
python3 -c "
import subprocess, json, sys
sys.path.insert(0, 'core/intelligence')
from audit_layers import classify_path
from pathlib import Path

# Get pack file list
result = subprocess.run(['npm', 'pack', '--dry-run', '--json'], capture_output=True, text=True)
pack_data = json.loads(result.stdout)
pack_files = [f['path'] for f in pack_data[0]['files']]

# Classify each
repo = Path('.')
violations = []
for f in pack_files:
    layer, reason = classify_path(repo / f, repo, is_file=True)
    if layer not in ('L1',):
        violations.append((f, layer, reason))

if violations:
    print(f'FAIL: {len(violations)} non-L1 files in pack:')
    for f, l, r in violations[:20]:
        print(f'  [{l}] {f} — {r}')
    sys.exit(1)
else:
    print(f'PASS: All {len(pack_files)} pack files are L1')
"

# Step 4: Verify 0 DELETE (SYNC-03)
python3 core/intelligence/audit_layers.py 2>&1 | grep "DELETE"
```

If the verification script reports non-L1 files in the pack, investigate whether:
- The files field includes a directory that contains L2 content → fix by using more specific entries
- The audit classifier is wrong → this would be a Phase 10 regression, flag it

If the pack output contains files NOT in the audit at all (e.g., npm auto-generated files), those are expected and should be ignored.
  </action>
  <verify>
    <automated>python3 core/intelligence/sync_package_files.py --print > /dev/null && npm pack --dry-run --json 2>/dev/null | python3 -c "
import json, sys
sys.path.insert(0, 'core/intelligence')
from audit_layers import classify_path
from pathlib import Path
pack = json.loads(sys.stdin.read())
files = [f['path'] for f in pack[0]['files']]
repo = Path('.')
bad = [(f,classify_path(repo/f,repo,True)[0]) for f in files if classify_path(repo/f,repo,True)[0] not in ('L1','REVIEW')]
if bad:
    for f,l in bad[:10]: print(f'  [{l}] {f}')
    print(f'FAIL: {len(bad)} non-L1 files')
    sys.exit(1)
print(f'PASS: {len(files)} files all L1')
"</automated>
  </verify>
  <done>package.json files field is updated from audit-derived sync script, npm pack --dry-run produces only L1 content, and audit confirms 0 DELETE candidates remain (SYNC-03)</done>
</task>

</tasks>

<verification>
CHECK 1: Sync script is functional and idempotent
  python3 core/intelligence/sync_package_files.py --print > /tmp/run1.json
  python3 core/intelligence/sync_package_files.py --print > /tmp/run2.json
  diff /tmp/run1.json /tmp/run2.json
  Expected: no differences (idempotent)

CHECK 2: package.json files field matches sync output
  python3 -c "
  import json
  with open('package.json') as f: pkg = json.load(f)
  import subprocess
  result = subprocess.run(['python3', 'core/intelligence/sync_package_files.py', '--print'], capture_output=True, text=True)
  computed = json.loads(result.stdout)
  assert pkg['files'] == computed, f'Mismatch: pkg has {len(pkg[\"files\"])} entries, script outputs {len(computed)}'
  print('PASS: package.json matches sync output')
  "

CHECK 3: npm pack contains only L1 content
  (see Task 2 verify command)

CHECK 4: 0 DELETE candidates
  python3 core/intelligence/audit_layers.py 2>&1 | grep "DELETE  :    0"
  Expected: "DELETE  :    0 (  0.0%)"
</verification>

<success_criteria>
- sync_package_files.py exists and produces deterministic, idempotent output
- package.json files field is derived from L1 audit (not hand-curated)
- npm pack --dry-run shows only L1 content
- 0 DELETE candidates remain in audit
</success_criteria>

<output>
After completion, create `.planning/phases/11-package-sync/11-01-SUMMARY.md`
</output>
