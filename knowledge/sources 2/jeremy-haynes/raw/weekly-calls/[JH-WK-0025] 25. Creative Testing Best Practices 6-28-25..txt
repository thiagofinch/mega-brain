TranscriÃ§Ã£o Completa: 25 Creative Testing Best Practices 6 28 25
[00:00]
All right, welcome everybody. Uh, we got a few requests this week for creative testing best practices. Coincidentally, I actually have a YouTube video dropping today, too, where I lightly go through this subject. Naturally, here in the inner circle, we'll go way deeper than we ever will on YouTube.
[00:15]
So, let me drop the sauce on y'all. Um, to be clear, this was inspired from uh three different people. Number one, we got Jacob in the inner circle, Jordan and Jacob, business partners. They're one of the newest million-dollar month earners. Um, won't sit here and air out their revenue, but they're doing a lot more than that now. They're on an absolute tear.
[00:38]
And Jacob was asking at 35K a day for some creative best practices in terms of testing strategy. I'll go through some of what I had talked to him about. Um, that way we have some lessons for the scaled level. In addition to that, I'm going to talk specifically about what you do at lower levels of spend.
[01:00]
Um, we had, I believe, two different people, I think it was Pascal and, uh, Tyler that had also asked about this topic this week. So I'm sure we'll find a lot of benefit in it collectively together. But without further ado, let's dive in.
[01:15] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Creative Testing Framework - Constants vs Variables"
NÃšMEROS: NÃ£o especificados
VISUAL: Diagrama mostrando separaÃ§Ã£o entre elementos constantes e variÃ¡veis em testes de criativos, com setas indicando isolamento de variÃ¡veis
[01:17]
So there's a whole bunch of stuff that we do when we officially test. Okay, so testing is I want to be very direct when I say this. All contingent upon the amount that you spend per day. Okay, do me a favor. Drop in the chat real quick so I have a general idea of what range you guys are all at collectively.
[01:38]
Let me see if I can notice any patterns. What are you realistically spending per day inside of an ad account when you look to test? 2.5K, 10K a day, 2K a day, 2K a day, about 2K, 500, 1 to 2K. Yeah. So, right away I see a pattern somewhere around it seems like about 2K a day for spend per day or like creative testing in general.
[02:05] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Constants & Variables in Marketing"
NÃšMEROS: NÃ£o especificados
VISUAL: Conceito visual de maioria de variÃ¡veis como constantes vs. uma variÃ¡vel especÃ­fica de teste, layout comparativo
[02:08]
So, let me start with this. All right. Number one, just a very core lesson inside of marketing in general is we have constants and we have variables. Okay, we have to determine at a given time what we are going to keep within the constants category and what we are going to put into the variables category.
[02:25]
We generally will obviously have a majority of our total variables as a constant and we will have a specific testing variable. Okay, that is our variable that goes against our controls.
[02:38] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Creative Testing Components: Body Copy, Videos/Images, Headlines"
NÃšMEROS: NÃ£o especificados
VISUAL: TrÃªs caixas distintas mostrando os componentes principais de teste criativo: copy do corpo, vÃ­deos/imagens, e headlines
[02:40]
So it's obviously very important to understand like creative testing for me typically means that I'm focused on the ad level with a few key components. So those key components being we have our body copy, we have our videos and images and then we have headlines. This is generally what I consider creative testing, okay?
[03:00]
Are these specific variables here. But I want to be very direct in saying we can obviously roll a lot of these same principles and lessons over to our landing pages as well, which I'll also lightly touch on throughout this video, but I'll touch on that towards the end.
[03:15]
Let me start with the actual creative testing first on platform. Um I'm going to isolate this specific lesson towards Facebook and Instagram. The same general lessons apply on Google Adwords and Tik Tok to be clear.
[03:28] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Video vs Image Ratio Strategy"
NÃšMEROS: 70% vÃ­deos, 30% imagens
VISUAL: GrÃ¡fico de pizza mostrando proporÃ§Ã£o 70/30, Ã­cones de vÃ­deo e imagem, setas indicando intenÃ§Ã£o do usuÃ¡rio
[03:30]
So again, look for the principles within the lessons if you're spending elsewhere. Um to be clear, the very first thing that I want to make known is the ratio between videos and images. Okay? I generally find that from videos, I have a much higher intent from the user that goes to the landing page or fills out the form or does whatever action that I'm specifically trying to get the person do to do comparatively to images.
[03:55] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Interest Spectrum: Curious â†’ General Interest â†’ Convicted"
NÃšMEROS: NÃ£o especificados
VISUAL: Espectro horizontal mostrando nÃ­veis de interesse do lead, com zona de "leads frios" marcada, gradiente de cores do frio ao quente
[03:58]
Okay, I'm constantly thinking with this interest spectrum, which you guys likely hear me talking about a lot because it is the number one variable in my opinion that dictates whether I get good leads or bad leads. Okay? And what I'm always attempting to do is I'm attempting to drive leads that have higher levels of interest.
[04:18]
Generally, anybody that comes in above a convicted level is more probable to convert because they're obviously already sold. When we get people that are in the ranges of like curious or even general interest, these are leads that people typically complain about.
[04:35]
So this specific category and below is where you get feedback like leads are cold, you know, leads are coming through like too early in the sales process. And from my experience, I want to be really clear when I say this, what has a probability to get general leads and highly interested leads are videos more than anything else.
[04:55] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Video vs Image Performance Comparison"
NÃšMEROS: 70% vÃ­deos, 30% imagens
VISUAL: Tabela comparativa mostrando: VÃ­deos (menos alcance, mais intenÃ§Ã£o, leads qualificados) vs Imagens (mais alcance, menos custo, menos tempo, menor intenÃ§Ã£o)
[04:58]
Okay? So my ratio of where I typically end up is I typically do 70% video and I do about 30% images. Images typically have more reach and less time for less dollars. Whereas videos have less consumption but higher intent leads and also limited reach compared to to images.
[05:20]
Um, I want to again be super direct in saying I have a higher ratio of videos comparatively to images due to the fact that I want more leads coming through at the highly interested level or at the very least the general interested level.
[05:35]
There's a common lesson inside of marketing which is, you know, the the ad's intention is to get people to go to the page and then the pages intention is to get people to take whatever the action is on that page. And and people break it down like step by step, you know, and this specific principle applies to like writing long form copy as an example where people say, you know, the point of the first line is to get them to read the second line and the point of the second line is to get them to read the third line, yada yada.
[06:00]
I don't give a fuck about that rule. I like to again take the concept of videos and apply it to the ad and I like to assume that I can do a lot of selling right then and there. If I just simply peak curiosity, which is what an image is more probable to do when somebody goes to the page, they are far less probable to convert at a higher level of interest.
[06:22]
So again, I'm much more probable to get a lower quantity of people through. So anyway, from the perspective of, okay, I'm going to have 70% images. I'm sorry, I'm going to have 70% videos and 30% images.
[06:35] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Creative Surplus Strategy"
NÃšMEROS: NÃ£o especificados
VISUAL: Ãcone de estoque/reserva, setas mostrando fluxo de criativos, conceito de "far more than needed"
[06:38]
I therefore need to have really a great video testing strategy in mind for how I'm going to get into what we call a creative surplus. Okay, creative surplus is the goal due to fatigue. Okay. My goal, my intention when I go make videos is to make far more than the actual quantity that I need. You understand?
[06:58]
This is really important. I always want to when I shoot make not just a little more, far more than what I actually need. Okay? Some of you guys are straight up tweakers when you make creative. you just say, "Oh, I instantly got to go throw it out there and get it tested." That's a shit take.
[07:18]
Okay? I don't do that. I never do that. I don't want to I don't want to succumb to doing that. Okay? Because we are always going to be dealing with ad fatigue. Okay? And ad fatigue at different levels of spend dictates the time frame that we are probable to encounter ad fatigue.
[07:35] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Proactive vs Reactive Creative Management"
NÃšMEROS: NÃ£o especificados
VISUAL: ComparaÃ§Ã£o visual: lado esquerdo "reativo" (vermelho, caÃ³tico) vs lado direito "proativo" (verde, organizado com surplus)
[07:38]
Now, I don't want to be reactive. That's the point of the creative surplus. Creative surpluses keep me in a position where I'm proactive. Meaning, as soon as I see fatigue occur, I can just relaunch the campaign. I can swap in some new creatives and I'm pulling my cost per result back down to what it was pre- fatigue.
[07:58]
Okay. The whole other intention of a creative surplus is if a test fails, which there's a really high probability it fails, I need to be able to pull those out, swap in new ones, and not be attached to things.
[08:10] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Detachment Principle: I Don't Want to Be Right, I Just Want to Make Money"
NÃšMEROS: NÃ£o especificados
VISUAL: Frase destacada em negrito, Ã­cone de cifrÃ£o, conceito de desapego emocional dos criativos
[08:12]
Okay, detachment is a very critical lesson when it comes to creative testing. Okay, I don't want to go into testing with the idea of like, oh, this one's going to work better than that one. um I'm biased towards this specific one or that specific one. I don't want to do that shit okay?
[08:28]
Because that's going to heavily bias my outcomes and my testing in general. You want to operate with this principle in mind, okay? I don't want to be right. I just want to make money.
[08:38]
Is Sanjay on this call? Sanjay, unmute yourself real quick.
[08:42]
SANJAY: Yes, sir.
[08:44]
What was your play rate on that VSSL you sent me just the other day?
[08:48]
SANJAY: like 85%.
[08:50]
And what was the engagement rate?
[08:52]
SANJAY: Like fucking 6%.
[08:54] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Case Study: VSSL Performance"
NÃšMEROS: 85% play rate, 6% engagement rate, 40-60% engagement rate (benchmark histÃ³rico)
VISUAL: ComparaÃ§Ã£o de mÃ©tricas, setas vermelhas mostrando desempenho abaixo do esperado
[08:55]
And when I immediately responded to you and I said, "Sanjay, this VSSL is clearly ass. It needs to be replaced." What was your response?
[09:02]
SANJAY: My response was like, "Fundamentally, it's good." Like the if
[09:08]
your response was, I on average get a 40 to 60% engagement rate from this exact VSSL framework. It's got to be good, right? And I said, "Who gives a fuck about that?" Like the stat is telling you what the stat says.
[09:22]
So Sanjay's bias in that example was, "Well, oh, Jeremy, I've done this exact framework for a bunch of other people and it's worked really well." Just because it's not working here, like it must be the market that's wrong, right?
[09:35]
Like how how would you benefit from holding on to that belief as an example, Sanjay? Like would that benefit you in any way, shape, or form to hold on to the idea that that VSSL is good?
[09:45]
SANJAY: No. It's going to keep me in poverty.
[09:48] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "The Monkey & The Nut Analogy"
NÃšMEROS: NÃ£o especificados
VISUAL: IlustraÃ§Ã£o metafÃ³rica: macaco segurando noz, preso, tigre se aproximando - representa apego a criativos/estratÃ©gias que nÃ£o funcionam
[09:50]
Exactly. It's going to it's it's it puts you in a in an impoverished position. Okay, we can't have that. We don't want that. And that's the monkey grabbing the nut in the stump, not being able to pull its hand out because the the fist is too big to get it out of the hole it stuck it into, and a tiger coming along and killing you. All you got to do is let go of the nut sometimes.
[10:12]
So again, detachment, creative surplus. I don't want to be right. I just want to make money. That's the belief behind creative testing because we are all probable to have a low hit rate.
[10:25] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Hit Rate: The Undermined Metric"
NÃšMEROS: 6.6% hit rate (exemplo de Jacob), sub 10% hit rate (maioria dos casos)
VISUAL: GrÃ¡fico mostrando baixa taxa de sucesso em testes criativos, maioria vermelha (perdedores) vs minoria verde (vencedores)
[10:28]
Okay, the hit rate inside of creating inside of creative testing is a very undermined characteristic of creative testing. Okay, most people never actually quantify this. So, as an example, when I was talking to Jacob and he's spending upwards of $35,000 a day and and you know, he was kind of like complaining about this fact that, you know, hey, I feel like when I, you know, do my testing, like I'm doing something wrong because I'm I'm moving such a low quantity of them like to winning campaigns.
[11:00]
Like, I'm pulling out very few winners. And I was like, dude, that's totally normal. That's like a very regular thing inside of creative testing cycles. Okay? you are more probable to have a low hit rate than a high hit rate.
[11:15]
Hit rate meaning how many actually are winners. How many drive profitable high statistics like high click-through rates, low CPMs, uh you know, great conversions, great ROI data, like again you're going to have a much fewer quantity of winners than you do uh you know act like you're not going to have a majority simply.
[11:35]
But that's all I'm trying to say. So anyway, I digress. Point I'm trying to make is very simple. Okay, we have to quantify our hit rate. If you actually go into your account and you just sit there and you count the quantity of losing ads that you've tested and you count the quantity of winning ads and you just divide the winners by the total amount that you've tested, in most instances, you're going to see like a sub 10% hit rate.
[12:00] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Hit Rate Math: Jacob's Case Study"
NÃšMEROS: 6.6% hit rate, 100 criativos testados = 6-7 vencedores, 200 criativos = 12-14 vencedores
VISUAL: EquaÃ§Ã£o matemÃ¡tica visual, 100 criativos divididos mostrando apenas 6-7 em verde (vencedores)
[12:02]
Okay? Like as an example for Jacob, his was about 6.6%. So from that perspective, when you talk about a creative surplus and you try to quantify like what being in a surplus means, if he had a hundred creatives, okay, that he specifically tested, there would be at a 6.6% hit rate, literally six to seven winners.
[12:25]
Okay, think about that. So if we have to test a hundred individual creatives to pull six or seven winners out of that shit it's like we have to therefore justify like an efficient way to be able to spend through a hundred creative tests to pull out the six or seven winners.
[12:42]
Okay, that's the whole goal is to determine our creative our our total creative hit rate. Okay, once we have our creative hit rate, we have a mathematical number in mind that we can back into and if we work in multiples of 50 or if we work in multiples of 100 and again it's really contingent on upon the amount we actually spend per day.
[13:02]
Okay, so let me give you some examples. So at at his level at 35k a day in spend, it's like he can absolutely justify testing 100 creatives. Again, not only can he justify it, it's like realistically he could probably cycle through more.
[13:18]
So if he if he went through 200 creatives as an example, he could get 12 to 13 or 14 total winners, you know, and hey, if he wants more winners sooner, like that, that'd be what he does.
[13:30] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Budget-Based Creative Testing Volume"
NÃšMEROS: $35K/dia = 100-200 criativos, $2K/dia = volume reduzido
VISUAL: Escala comparativa mostrando diferentes nÃ­veis de gasto diÃ¡rio e volume de testes correspondente
[13:32]
In comparison, let's use the example that you're spending what most of you guys sit here and said in the chat, which is you're at like 2K a day. If you're at 2K a day, like, can you do a hundred creatives? It's like, well, technically you could, but I wouldn't.
[13:48]
What I like to do is I like to break it down in a really simple way. Okay, so here's the goal. Let me explain the goal first, and then we'll back into how you can justify what you need to spend daily to pull out your winners based on your hit rate.
[14:05] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Campaign Structure: CBO vs ABO Strategy"
NÃšMEROS: 4 ad sets, $500/dia por ad set (exemplo com $2K/dia total)
VISUAL: Estrutura hierÃ¡rquica: Campanha no topo, 4 ad sets abaixo (Ad Set 1, 2, 3, 4), todos com mesma configuraÃ§Ã£o
[14:08]
Okay, so here's how I typically structure my creative testing campaigns. So, I'll have the campaign. I will then have my ad sets. Okay? And notice how I said ad sets. So it's like I'll have ads set one, I'll have ad set two, I'll have ad set three, and then I'll have ad set four.
[14:25]
Again, thinking in constants and variables. The very first lesson in which I communicated, one of my constants in this case is going to be the ad sets. So these are all going to have the same targeting, the same placements, like everything's going to be the exact same within all four of these individual adsets.
[14:42]
In this example, I'm going to be doing, in this case, ABO instead of CBO because I don't want my campaign to bias towards a specific batch of ads within these adsets. I want to make sure that each one of my adsets gets equal distribution.
[15:00] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Ad Set Configuration: Equal Distribution Strategy"
NÃšMEROS: 25 ads por ad set, 100 ads totais, $500/dia Ã· 25 ads = $20/ad
VISUAL: Breakdown matemÃ¡tico visual, caixas representando 25 ads dentro de cada ad set
[15:02]
Okay. Now, here's on the adset level what we're going to do within it. Okay. So, to be clear, again, you could do whatever targeting you want. Let's say that you have out of all the targeting options that you've tested like a lookalike stack, broad um an interest stack, maybe warm audiences, pick whatever you want, but make sure you keep constants.
[15:25]
Okay? So, whichever whichever specific thing you want to bias towards, I don't give a fuck about that. Like, you could have all of these be lookalike stacks if you want. You could have all these be interests. You could have all of them be broad.
[15:38]
I don't typically bias towards one thing or the other. I just look at what's winning in the account and I test what I want to then take my winning creatives and throw it against in terms of audiences.
[15:50]
Maybe after I get some winners from this creative test with whatever audiences worked best, then I go do my creative uh winners against different audiences and I do an audience split test next. But remember, we're trying to we're trying to have a very very small minority of things that are variables here.
[16:08]
We're trying to have a majority of things be constants, okay? So, pick something that works. Doesn't really matter. And then, as in this example, let's say that you had 2K a day. Okay? Your 2K a day would be divided up per adset in this example. So, I would have $500 per ad.
[16:30]
Okay? And I'm trying to evenly distribute it. Now the question becomes, how many creatives can I realistically test within these ad sets and get distribution to determine an actual winner and to to determine losers. That's what actually determines the quantity of creatives that we're going to get to test is budget.
[16:50] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Dynamic Creative vs Individual Ads Decision Matrix"
NÃšMEROS: 25 ads por ad set, 100 ads totais
VISUAL: ComparaÃ§Ã£o lado a lado: Dynamic Creative (pros/cons) vs Individual Ads (pros/cons), setas indicando escolha recomendada
[16:52]
Okay, so let's say that I did want to do a hundred. Okay, inside each one of these ad sets, I would have 25 ads. Okay, now the question becomes, do I want to do dynamic creative or do I want to do individual ads?
[17:05]
Okay, individual ads meaning we're just going to set up manually, you know, this video paired with this body copy and this headline. Um, in this case, when I typically do creative testing, what I'm going to choose to do is I am going to choose to do individual ads when I'm attempting to test like this.
[17:25]
Okay, there is a different testing strategy that I'll talk about here in a moment for even smaller levels of spend where I will leverage dynamic ads rather than individual ads. However, when I'm testing this quantity of creatives, if I leveraged dynamic ads and I had too small of a budget, what you're realistically going to encounter is an immediate bias towards a specific creative combination and most of the other creatives aren't actually going to get results.
[17:50]
The other huge liability that we have when running dynamic creative is very unfortunate because this glitch still exists in in a majority of accounts. Um you're actually not even going to be able to see the results of the dynamic performance.
[18:05]
So when you do the breakdown tab and you click on buy dynamic creative asset and you select like images, videos, the headlines, the body copy, in some instances it doesn't even populate the results or it doesn't populate at all.
[18:20]
And when we're doing creative testing, like we can't run that liability. We obviously need to be able to see the individual results of each one of these specific creatives. So individual ads is what I bias towards.
[18:32] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Distribution Strategy: Why Multiple Ad Sets?"
NÃšMEROS: 4 ad sets x 25 ads = 100 ads totais, opÃ§Ãµes: 8 ad sets x 12-13 ads
VISUAL: Diagrama mostrando divisÃ£o de ads em mÃºltiplos ad sets para garantir alcance igual, setas mostrando alcance distribuÃ­do
[18:35]
And this is what I want to see when I set it up this way. Okay, this is really important you understand this. So let's go under the assumption again in this hypothetical scenario that again I have my campaign. I have four adets. They're all targeting the exact same thing. I have 25 total ads inside of each one of these ad sets. Totaling a hundred altogether across all four of them.
[18:58]
And again, I want to be really clear when I say this. Why did we break them up into four ad sets instead of just one giant ad set with a 100 ads in it? It's because of distribution. I want to see a relatively equal distribution.
[19:12]
I want each one of these 25 ads within this ad set to get the reach that it deserves before it starts to bias towards winners. Okay, that's how I'm going to be able to determine winners and losers. I got to actually get reach on the individual ads.
[19:28]
So, you can set this up a little differently. And let me go through that real quick just to make sure we're all on the same page. So, let's say that I launch the campaign this particular way first. Okay. And I do not see equal reach. Okay, I might end up having to break it out even further than that.
[19:45]
Like I might have a total of eight ad sets. So I can take that 25 total ads per adset and break it down to like 12 to 13 ads per ad set. Again, all I have to do is just continue to break down my total quantity of adsets here until I get equal reach on the ad level.
[20:05]
Okay, that's what I'm looking for on the ad level. I want to see equal reach to determine winners. Okay, that's what I'm attempting to do. And it's really important you understand this.
[20:15] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Budget Scaling: Jacob's $35K/Day Example"
NÃšMEROS: $35K/dia, $5K/dia para teste, 100 ads em 1 ad set possÃ­vel, 50+50 em 2 ad sets se necessÃ¡rio
VISUAL: Escala de orÃ§amento mostrando flexibilidade de estrutura baseada em budget disponÃ­vel
[20:18]
Okay, I can and have had success doing a campaign, one ad set with a hundred ads in it, but I had far greater budgets. So, as an example for Jacob spending 35k a day, I told him, I was like, "Listen, dude, if you want to throw like, you know, five grand a day at a test, as an example, you realistically could go campaign, single ad set, 100 ads in it, right?
[20:40]
And if you don't see equal reach, break it down to two ad sets. Campaign, two ad sets, 50 and 50 per ad set in terms of creatives, right?" And again, if I still don't see equal reach, then I'm going to four. You guys get what I'm saying?
[20:55]
Put a one in the chat if you're picking up what I'm putting down so far and you understand this because this is very critical. This is the structure of the campaigns itself.
[21:05] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Budget Distribution Math"
NÃšMEROS: $2K/dia Ã· 4 ad sets = $500/ad set, $500 Ã· 25 ads = $20/ad
VISUAL: CÃ¡lculo matemÃ¡tico visual em cascata, mostrando divisÃ£o de orÃ§amento por nÃ­veis
[21:08]
Okay. Now, let's just tie in the budget. Thank you very much, guys. Appreciate that. Let's talk about the budget side of things. So, again, if I'm at 2K a day and I have $500 per ad and I do want to test a 100 creatives, I could set it up that way for sure.
[21:25]
Okay? I could I could set it up just like how we just discussed with a single a single ad set, two ad sets, four ad sets, eight ad sets, whatever. Okay, it's more probable. Let me be really clear on this. It's more probable that at lower budgets per day, you need far more adets to actually get individual reach on those ads.
[21:45] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Budget vs Time Relationship"
NÃšMEROS: NÃ£o especificados numericamente
VISUAL: GrÃ¡fico de balanÃ§a: budget alto = menos tempo de teste, budget baixo = mais tempo de teste, setas inversamente proporcionais
[21:48]
Okay, you understand? Very important you get that. Okay, higher budgets I can consolidate. Lower budgets I have to spread it out further to get the reach. The other critical thing to understand is at lower budgets I also have to endure the test for more time.
[22:05]
Okay. So higher budget per day I can test through shorter windows of time. Okay. Higher budgets per day less time. Lower budgets per day more time. Okay. Now what is the amount of time that we have to wait and endure? That becomes the next question.
[22:22]
Okay. So, for me, I like to have a specific amount of people that clicked through on those ads or that the ads were served to before I become conclusive about my tests. Otherwise, I'm kind of prematurely shutting shit off that might have turned into a winner. Think about that.
[22:40] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Spend Per Ad Calculation Example"
NÃšMEROS: $500/dia Ã· 25 ads = $20/dia por ad (teÃ³rico), na prÃ¡tica: alguns ads $50, outros $5
VISUAL: DistribuiÃ§Ã£o real vs teÃ³rica de gastos, grÃ¡fico mostrando variaÃ§Ã£o de spend por ad
[22:42]
So moral of the story is if I'm spending as an example 2K a day across these four adsets in this example. So 500 a day per ads set and I have 25 ads. Okay, it's pretty simple math. So let's take our $500 per day and let's divide it by 25. That leaves $20 to realistically be spent per ad within that ad set in that example.
[23:05]
You understand? So in this case again I'd have 500 a day per ad set. Okay. and I have a total of 25 ads within that ad set. So, if we divide those two, what we end up getting is $20 a day essentially going per ad.
[23:20]
Now, we all know it's not going to actually spend equally between each 20. Like, it's not going to just spend 20 bucks an ad. You know, that's not actually probable to play out. You understand? What's more probable to play out is like one or two or five of those specific ads might get, you know, $50 of that and, you know, a handful of those ads might get five.
[23:42]
Okay? Now, it's very important to understand this. Does that mean that the things that only got $5 are losers? Right? Not necessarily. But it means that there was a clear algorithmic bias towards a specific set of creatives there, right?
[23:58] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Algorithmic Bias: Good or Bad?"
NÃšMEROS: NÃ£o especificados
VISUAL: Ponto de interrogaÃ§Ã£o grande, balanÃ§a mostrando bias algorÃ­tmico como indicador positivo (maioria hook)
[24:00]
What does that mean? When we see a clear algorithmic bias towards us to towards a handful of ads, why does that happen? Okay, because to be clear is you essentially have to ask this question. Is is the algorithmic bias is it bad or is it good? Right? What what signal is it giving us when it has an algorithmic bias?
[24:20]
Okay, so for me there's a few things that I consider. Number one is if I see a clear algorithmic bias, which is far more probable to happen than just an equal distribution, I do not interpret it as bad. I interpret it as good.
[24:35]
I look at that and I say, "Okay, so in terms of platform distribution and what this giant alien level technology I have access to here is showing me, I'm going to come to the conclusion that that specific ad is probable to do better with a broader demographic of people."
[24:52] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Majority Hooks vs Tiny Hooks"
NÃšMEROS: NÃ£o especificados
VISUAL: ComparaÃ§Ã£o visual: Majority Hook (grande, ampla distribuiÃ§Ã£o, menos fadiga) vs Tiny Hooks (pequenos, distribuiÃ§Ã£o limitada, fadiga rÃ¡pida)
[24:55]
Okay? Because I think in hooks. I think in majority hooks. Okay? What I'm constantly looking for is the specific hook that is going to distribute to the broadest demographic of people and convert the broadest demographic of people.
[25:12]
Typically from there, we have a bunch of small hooks. We have the second place hook, we have our third place hook, and typically with a majority hook, we're going to see a larger distribution, and we're typically going to have less fatigue.
[25:28]
like we're going to endure fatigue cycles for longer with majority hooks. With these tiny hooks, we have a much higher probability to see fatigue rapidly. And we also, unfortunately, have a very high probability to only reach a certain subset of people that the ads are going to be responsive with before we see fatigue.
[25:48]
Okay? We're going to have scale limitations on the smaller hooks. So all these little guys here, this might be the stuff that you see gets less reach. These little tiny hooks.
[26:00] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Tiny Hooks Characteristics"
NÃšMEROS: $8K-10K+/dia (potencial de escala de majority hooks)
VISUAL: Lista de caracterÃ­sticas: fadiga mais rÃ¡pida, menos escala, transformam-se rapidamente em campanhas fundamentais
[26:02]
Okay. So tiny hooks, simply put, they fatigue faster. In addition to fatiguing faster, they have less scale. Okay, these turn into foundational campaigns extremely quickly when you have tiny hooks.
[26:15]
when you have majority hooks like these are the ones that you can get to like 8K a day, 10K a day plus and sure they might eventually hit a ceiling in that range or maybe a little higher than that but depending obviously on what the hook is essentially dictates the scale potential.
[26:32]
So again algorithmic bias is it bad? No, I don't think so. I think it's a clear indication that I likely have a majority hook that I just found. Okay, that's what I start to think right away.
[26:45]
the bias of the ones that get less distribution. To me, that simply means these ones have less reach because they're tiny hooks. They don't appeal to as many people.
[26:55] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "The Wrong Mindset: Blaming the Platform"
NÃšMEROS: NÃ£o especificados
VISUAL: X vermelho grande sobre frase "Facebook sucks", seta apontando para "Blame Your Structure"
[26:58]
Okay? Now, here's here's the interesting thing. Uh, everybody determines and looks at data a little differently. Okay? So, let me be really clear when I say this. You're going to potentially see a tiny hook that has a baller click-through rate, maybe even it got a cheap cost per result, and you're going to be like, "Oh, dude, this one's not fucking getting money spent on it, but like the stats are awesome, and Facebook's not distributing dollars on it."
[27:25]
You know, Facebook fucking sucks. And like, you're a dumbass if you say that. Like, you have no idea what the fuck you're talking about, okay? You have no idea what you're talking about and how things work if you say that, okay?
[27:38]
Cuz here's what you can do if you see that occur. It's like don't blame the platform, blame your structure that you used, okay? Because to be clear, you might have spent too little and put too many ads into what you spent too little on.
[27:55]
And so although yes, it might have gotten good distribution or very little distribution and a great cost per result and a great set of statistics, but some other majority hook crushed it. it got more distribution. It it was applied to more people.
[28:10] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Layered Creative Testing Strategy"
NÃšMEROS: NÃ£o especificados
VISUAL: Camadas mÃºltiplas de testes, setas mostrando fluxo: teste inicial â†’ breakout de majority hooks â†’ breakout de tiny hooks
[28:12]
So, what you have to do is you have to have layers to your creative testing. Okay, this is really important to understand. This is truly one of the number one ways that I see people go full retard instead of having a good creative testing strategy.
[28:28]
Okay, so here's how it works. If I see something that gets squashed by something else that had larger distribution but worse stats, I'm not going to treat it like a winner or loser scenario yet.
[28:42] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Breakout Strategy: Majority Hooks vs Smaller Hooks"
NÃšMEROS: Exemplo: 5 ads (ad set 1) + 7 ads (ad set 2) + 3 ads (ad set 3) + 11 ads (ad set 4) = 26 ads para breakout
VISUAL: Dois caminhos distintos: Majority Hook Testing Batch (mais orÃ§amento) e Smaller Hooks Testing Batch (menos orÃ§amento)
[28:45]
I'm going to treat it like this goes into my majority hook testing batch and this goes into my smaller testing batch. Okay? So, my smaller hooks, I'm going to break them out. So, you have to have what's called a breakout strategy. Okay, you have to have a creative testing breakout strategy.
[29:05]
So, let me give you an example back to this. Okay, so let's use the example that I have 100 ads in total. And again, I have four adsets, 25 ads per ad set. And let's say that out of the 25, five out of 25 got most of the reach.
[29:22]
Okay? And like two to three days passed in this case at 500 a day. Let's use the example that we spent about a thousand to 1,500 bucks. Okay, in terms of our test, well, do we determine the 20 to be losers and just kill them and throw them in the trash bin?
[29:40]
No. What we want to do is is we want to take these five that are here and we want to break them out. So, we want to take these five and we want to put them into their own campaign.
[29:52]
Now, here's the thing. It's not just going to be five, okay? Because keep in mind, this is only assuming 25 here. You have a total of a hundred being tested in this example. So, let's say that in the second ad set you have seven. Let's say that in the third ad set you have three. And let's say that in the last ad set you have 11.
[30:15]
Okay? Each one of these batches of what got the most reach is now going to go into its own testing campaign. So then we end up creating a new creative campaign where we're going to test those larger reach. So I call this typically like my reach testing campaign.
[30:32] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "New Campaign Structure After Breakout"
NÃšMEROS: 26 criativos totais no exemplo de majority hooks, 74 criativos restantes para tiny hooks
VISUAL: Estrutura de duas novas campanhas: Reach Testing Campaign (maior orÃ§amento) e Creative Testing Campaign (menor orÃ§amento)
[30:35]
Okay? And then this remaining quantity of of quote unquote losers that just didn't get the distribution and reach. What we're going to do with those is the same kind of concept. We're going to take those and we're going to put those into a creative testing campaign.
[30:50]
So we're going to do another creative testing batch. Okay? So we're going to do a new creative campaign and treat this just like you did for this one up here. Okay? This one instead of having a hundred will just have less than a hundred, but you want to treat it the same.
[31:08]
Okay, you still want to spend you're going to typically put more dollars towards this because there's more creatives. This typically, although still a creative testing campaign, typically has less spend that has to go towards it. Simply put, because there's less creatives.
[31:25]
And again, from a constants and variables perspective, you're still attempting to have a very clear testing strategy here where the adsets are all going to target the exact same audiences that they were just targeting. Okay?
[31:38]
I have to keep things damn near the exact same for this to actually show me the right data that I need from it. You understand? So, to be clear, it's like I have to have the campaign set up the same. I have to have the adsets broke down in a way where they're still competing against one another now with the same audiences.
[31:58]
And I just am breaking out my adsets into potentially a larger batch for this technical more reach one. I might have a total here of let's say okay so I got I got 10 15 11. So I got 26 total creatives here in this hypothetical example. Right?
[32:18]
It's like I could put those realistically inside of one ad set. I could also, if I wanted to, do two ad sets. Real quick, I'm going to test some of you. Put the answer in the chat.
[32:30]
So, if I have 26 total ads in this example that I need to test, okay, am I going to do one ad set or am I going to do two ad sets? What determines the answer? Somebody drop it in the chat or feel free to unmute yourself and answer the question. What would determine the answer?
[32:48]
Budget. Everybody answered budget. Very good, everybody. You guys are all paying attention and that is absolutely the correct answer.
[32:58] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "The Second Determining Factor: Reach Distribution"
NÃšMEROS: 26 ads em 1 ad set, se nÃ£o houver alcance igual â†’ 2 ad sets com 13 ads cada
VISUAL: Ãrvore de decisÃ£o: Budget â†’ Alcance Igual? â†’ Se nÃ£o, dividir em mais ad sets
[33:00]
There is one other answer though that I want to make very clear. If I do have what I determine to be enough budget, okay, I'm going to throw a scenario out there and I want to make sure you guys answer this the right way. This is a question. Pay attention.
[33:15]
Okay, let's say I put them all in one ad set because I mean I just fucking ran 25 ads in one ad set. Why wouldn't I just do one adset here? Okay, sure. Budget could determine that right away. That could determine that. Absolutely. Nobody's wrong.
[33:32]
Okay. However, let's say I put them all in one ad set because I do determine I have enough budget and I still don't get equal reach. Some of them don't get reach at all. Now, what do I do from there? Somebody tell me in the chat or unmute yourself. What do I do from there?
[33:50]
If I put all of these 26 in one ad set, I determined I have enough budget. All these ones, by the way, they just got the majority reach in the previous test. Now, some of them aren't getting any reach at all. What do I do?
[34:05]
There you go. you and take the ones that didn't get reached, put them in their own ads set. That's exactly right. Bang.
[34:12]
So, it's like from that point, that's what that's what determines it up here in terms of the total adsets. Like, what's the difference between only having one ad set, two adsets, four adsets, eight adets? Really, the answer is besides budget, is what is in there getting reach?
[34:30]
Right? If it's not getting reach, I'm going to break it out further, right? So in this example, I might end up with two ad sets that have 13 ads in it each. I might end up with one ad set because all 26 got relatively equal distribution. Right?
[34:48] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Equal Reach Timeline: Temporary vs Bias"
NÃšMEROS: Alguns dias ou algumas horas atÃ© bias ocorrer
VISUAL: Timeline mostrando distribuiÃ§Ã£o igual inicial â†’ bias algorÃ­tmico aparecendo ao longo do tempo
[34:50]
Now, here's the thing. You see equal distribution for a little bit of time and then bias occurs. Really important you understand this right here. Okay, I'm only going to see equal reach for a short duration of time. It might be like a couple days. It might be even just a couple hours and then I start to see a bias.
[35:12]
Okay, so let me make really clear what I'm attempting to say for when it's appropriate to break out the adsets into more adsets with less creatives per ad set. If I'm just not seeing reach at all or if I see like a literal couple dozen people that the ad reached.
[35:30]
So, you know how sometimes when you look at an ad, it literally has like a couple dozen people that it reached and like maybe like tens of cents that was spent on it. In that example, that's what I'm saying. that that counts as no reach.
[35:45]
Okay, I want to see that like it reached a couple thousand people realistically. That's like the deterministic variable that matters a lot. Now, I have to remind you at this point, you have a very low hit rate of what's actually going to be classified a winner, right?
[36:02] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Losers Deserve Reach Too"
NÃšMEROS: Alguns milhares de pessoas alcanÃ§adas = mÃ­nimo para determinaÃ§Ã£o
VISUAL: Conceito visual de dar chance justa aos "perdedores" antes de classificÃ¡-los definitivamente
[36:05]
Losers still deserve reach. You understand? Losers still deserve reach. Okay? But the winners are going to stand out based on obviously the winning set of statistics that you're looking for.
[36:18] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Winning Statistics Priority Order"
NÃšMEROS: NÃ£o especificados numericamente
VISUAL: Lista hierÃ¡rquica: 1) ROAS, 2) Cost Per Result, 3) Link CTR, 4) CPM, 5) Thumb Stop Rate
[36:20]
Winning sets of statistics typically the way I judge it above all else is obviously ROAS. Okay. But I'm probable to just get cost per result first before anything else. So I'm really judging more than anything in the short term on my cost per result being cheaper.
[36:38]
I'm also looking at uh click-through rates, link click-through rates. Outside of that, I'm also looking at specific variables again like ROAS, cost per result, link click-through rate, sometimes CPMs, thumb stop rate.
[36:52]
Uh, I just look at the bigger statistics first. I don't try to like nerd out and look at all I'm advertiser, okay? I don't look at all the little fucking nerdy statistics until I actually need to look at them and they matter. Okay?
[37:08]
Now, just to be clear, at this point, you started off with a creative testing campaign. You might have had one ad set. You might have had fucking 12. Okay, at that point, you likely saw some specific ads that started to have an algorithmic bias. That's not bad. That's good.
[37:25]
That means that we're now ready for phase two. Phase two means we employ our BL our breakout strategy. Okay, our breakout strategy is where we take the the ones that got the most reach that suffocated the others. We put them all up against each other. Let's see if they actually hold true and all still get reach.
[37:45]
That's not probable to be the case. What is probable to be the case is is that our quote unquote losers are either minority hooks or just got suffocated by something that was bigger than it in the eyes of the algorithm.
[38:00]
You're going to sometimes find that inside of this specific new creative testing campaign that you're going to have more creatives to toss into this campaign. Right now, ultimately, here's what you then have to ask yourself, and this is very important you understand this.
[38:18] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Two Options for Winners: A vs B"
NÃšMEROS: NÃ£o especificados
VISUAL: Duas opÃ§Ãµes distintas em caixas: A) Converter teste em campanha de escala, B) Puxar vencedores para campanha de escala existente
[38:20]
Okay? What's very important to understand is that you now have some specific creatives that are likely winning for you and and you have to determine one of two things. Are you going to a these are your options convert a test campaign into a scaling campaign? or B are going to pull winners into a existing scaling campaign.
[38:42]
Real quick, we're a bunch of advertisers here, minus a few of you that are newer to the game. Answer me this. How many times have you pulled a winner out of a testing campaign, threw it into a scaling campaign, and it did fuck all once you pulled it out? How many times does that happen to y'all?
[39:00]
It's like there's a really, really important rule of the game, which is if something's working somewhere and you pull it out of it and put it into something else, it it it does not have the same probability to just immediately work inside of whatever you pulled it into.
[39:18]
There is a probability that it'll work, but if you throw it into a campaign that's already been running, that already has a wellestablished bias, you're not going to see that new ad that you threw into it realistically do much at all. You might actually see worse performance on it.
[39:35]
And so therefore, if that does occur, what we have to do is option A, which is that testing campaign. We're just going to shut off every fucking loser inside of it that just isn't converting. And that campaign is now a graveyard of losers, but it's now our it's now what we're going to call a foundational campaign that we're attempting to scale up until we hit a ceiling.
[39:58]
And then we just throw it into the pile, maintain it, overcome ad fatigue when it occurs. And the goal becomes every testing campaign that I can that I have I'm essentially going to try to convert into a scaling campaign.
[40:15]
You see that typically in terms of probabilities has a higher probability to actually turn that clear set of winners that you've that you found into scaling winners. Right?
[40:28]
Unfortunately, and I just want to be clear when I say this, like I still have had a lot of success by following option B as well. I just want to be really clear on this. This is one of those scenarios where it's not like just a clear right and wrong. Like both of these technically work, okay?
[40:45] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Option A vs Option B: Success Probability"
NÃšMEROS: NÃ£o especificados
VISUAL: BalanÃ§a comparativa mostrando Option A (probabilidade ligeiramente maior) vs Option B (probabilidade ligeiramente menor), ambas viÃ¡veis
[40:48]
Both have probabilities to work. Okay? I just want to be clear. This has a slightly higher probability to work and this has a slightly lower probability to work, but both can work. I can't stress that enough.
[41:02]
Real quick, who here agrees? You've had that test. You've run the same thing. You've converted test campaign to winning campaigns. You've pulled you've pulled tests into winning campaigns. You've seen both work. You've probably also seen both not work. You know, it's like when one doesn't work, I just revert to the other. That's the moral of the story.
[41:22]
So, it's like that's essentially what we're trying to do. We're trying to run some test campaigns. We're trying to convert those test campaigns into winners or and this is a big or okay we pull the winners out and we put them into a scaling campaign.
[41:38]
And when we pull the winners out just in terms of the technicality here obviously we're going to shut them off in the testing campaign. So our testing campaign goes back to testing everything else within it.
[41:50]
If I convert an existing testing campaign, I am going to cut off every loser and I'm going to let the winners ride and I'm going to start scaling that testing campaign. Okay, this is really important you understand this at this point.
[42:05]
Um, let me just answer a few questions before we continue on this. I saw I think a few in the chat here.
[42:12]
Um, okay. Tyler says, "Is there a rule of thumb for a suggested budget? Your example is 500 a day for 25 ads. So, does that mean $20 a day is suggested per ad?
[42:25]
Yeah, Tyler. So, long story short, if you have a higher budget, you can test for a shorter duration of time. If you have a smaller budget, you have to test over a broader duration of time.
[42:38]
There's a whole bunch of rules that I covered relating to, you know, which one's likely most appropriate for you. It's it's really dependent upon the budget of the business for that month.
[42:50]
You you want to understand like your creative testing campaigns have a high probability of failure because of the hit rate. Your hit rate is typically going to be very low.
[43:02]
So, long story short, it's like, let's use the example, you only have like $100 a day to test or you only have $200 of day to test, you might do a different creative testing strategy in its entirety where you just instead of having 100 creatives, you might lower this total number down to like 10.
[43:22]
You know, again, simply put, there's two variables that matter most, right? My budget determines how many I get to test. Okay? So, if I have a higher budget, I can test more in a shorter duration of time. If I have a lower budget, then I can test less over that same shorter duration of time.
[43:42]
So, moral of the story is whatever your budget is for creative testing, whatever you determine it to be, that's kind of what determines this answer here. So, I mean, no, I don't really have like a always test at like this much per day. It's really dependent on where the business is at.
[44:00]
Uh, okay. So, Chris says, "So, it seems like all the adsets start out with equal distribution until the breakout starts to happen." Yes, that's correct.
[44:10]
Um Devin says, "Split to multiple adsets to get better reach." Uh yes, that's correct.
[44:18]
Um let's see here. Tyler says, "Same question. If the target audience is very small, how does this change things?" Uh typically just means less budget. So if you have a smaller audience, um you typically will test less creatives and you'll typically test with less budget.
[44:35]
Daniel says, "If you have a niche, do majority hooks still work well?" Yeah, absolutely. because there's always a majority hook for a given niche. Doesn't matter how big or small it is, there's always a majority hook.
[44:50]
Uh Timothy says, "Doesn't Facebook always bias towards a few ads within an adset? How can you force it to distribute spend equally?"
[45:00]
Generally at the very beginning is when it has relatively equal distribution and then typically within the first couple days, sometimes with even the first couple hours, depending on spend, it'll create a bias.
[45:15]
So, it's important to note if you see it start to bias heavily too soon to where some ads don't even get reach, then you need to break it out further into more adsets with less ads per adset. That's essentially how you control that more.
[45:30]
Um, you can even get to the point where you might have to justify like, you know, 40 adsets with like two ads in each as an example. You know, in the instance that your account's just absolutely fucking you.
[45:45]
Um, okay. Okay, Devin says, "With creative surplus, how do you know which to test initially versus which to keep in the bag?"
[45:55]
Yeah, great question, Devin. You can have a personal bias. Um, I t that's actually a great question. I I'm going to cover a lot here in a few moments about how to just get the creative surplus started in the first place, but just to answer your question directly now since you brought it up, I will determine a few that I think are really good and I'll determine a few that I think are not going to work and I'll test those against each other.
[46:20]
I'm trying to prove myself right and wrong at the exact same time. Um, so that's how I generally bias.
[46:28]
Okay, cool. Um, looks like I got them all. Uh, is there any other questions you guys have on this? Let me scroll down here to the bottom.
[46:38]
All right, Daniel says, uh, what's the best approach to moving to a winning campaign, especially if the winner is dynamic?
[46:48]
In that case, you'd you'd want to relaunch the campaign, Daniel, and you'd want to add those new assets to that winning campaign that you just duplicated out. And at that same moment, you also might want to pull out some of the losing dynamic assets based on the performance of that winning dynamic campaign up to that point. That's typically what I would do in that example.
[47:10]
Uh Timothy says, "The 25 ads you are launching, are they all different hooks or is it a combo different video?" Yeah, we'll go over that here in a moment, Timothy.
[47:22]
Uh Euan says, "What are your rules for determining whether an ad's a winner or loser?" Um spent 2x the target CPA with no conversion turn off. Are using automated rules or manually turning them off?
[47:35]
And so you can automate it and there's nothing wrong with automating it. I generally like to go in and just look at it manually though. Um to be clear, I am spending enough to see a couple thousand people reached and then I'm looking at the data.
[47:52]
So, if I see something that has in most instances a good cost per result, that's how I'm judging it more than anything else. If it already has ROAS, that's fucking sick. But in most instances, due to the sales cycles that we generally have for the businesses we work with, we are not able to determine ROAS immediately. We're able to judge things like the cost per result. So, that's really what we're judging more so than anything else when we do our creative testing.
[48:20]
Okay, Chris says, um, whenever we observe a result that requires us to take action, splitting out, scaling, etc., are we creating a new adset or editing the existing one?
[48:32]
Yeah, great question, Chris. So, if I'm converting an existing scaling campaign, I'm sorry, if I'm converting an existing testing campaign into a scaling campaign, then I'm just obviously doing that within the campaign. I'm not going to duplicate it out or anything.
[48:50]
um if I am quote unquote let's see splitting out Chris you want to give like a specific example of what you're talking about so I could be like more particular with you or do you just want me to go through all the scenarios?
[49:05]
CHRIS: Yeah 100%. I think I can uh maybe both is appropriate but I'll just uh expand on this. So imagine hey we see this one. Hey this is kicking ass within this dynamic adset. Two of these are killing it. I want to start to split out, right? The existing one I imagine the existing one is carrying on with the winners. And
[49:28]
so are you saying you're in the situation at that point where you have the A or B choice? Is that what you're saying? Ex.
[49:35]
CHRIS: Yeah, pretty much.
[49:38]
Yeah. So, it really just comes down to those two things. It's like I'm either going to keep the existing campaign going and just shut off all the losers and convert it to a scaling campaign or I'm going to turn off the winners, put those into the winning campaign, and now that campaign that had the winners pulled out of it, I can still run it from there as a creative testing campaign.
[50:00]
Or at that point, to be fair, I could duplicate it out without the winners that I just pulled out, let it start fresh and let it compete against each other again. You could do it either in the example of B.
[50:15]
CHRIS: Quick question on B though. Are you saying there's an existing ad set that's already, hey, this is a winner ads set and you're willing to add winning creatives to it while it's live?
[50:28]
That's correct. Yeah. And in that example, when you're quote unquote adding a winner to an existing campaign, you are in most instances going to be duping out whatever you're adding it into. That if that's the question you're answering, in most instances, I'm duping it out, adding it in. That way, it all starts fresh and they compete they compete against one another. And so,
[50:52]
CHRIS: if that helps, I'm trying to see all these scenarios, you know.
[50:55]
Yeah. So, in some instances, let me be clear because to your point, yes, there are a lot of um there are a lot of instances that you can come across here. And there's a lot of ways to be fair that work that also sometimes don't work. So, you want to be aware of all of them.
[51:12]
The other instance that you could do is is let's say you have a campaign, you have it optimized for adset budget optimization, and you want to add a new creative winner to that campaign. I'm just going to dupe out one of the existing adsets. throw that new set of winning creative into that ad set by itself, publish it, and if I don't see it perform, then I'm going and I'm con I'm going back to A. I'm going back to option A in that example.
[51:40]
If in that same exact hypothetical scenario, I go to add through option B a winning creative to an existing campaign that's a winner. I'm going to be duping out that ad set. I'm going to be adding that creative, that winning creative to that adset, publishing it. Hopefully, I see it work.
[52:00]
And then I'm just testing essentially all it's it's my scaling campaign. So I'm going to have a bunch of the adsets that target the exact same shit They all just have different creative in it in that example. Does that make sense?
[52:15]
CHRIS: So just be ready to dupe. When we do dupe, do we get rid of the or shut down the original that was uh that was dupes?
[52:25]
That's up to you. I mean to be fair, sometimes you'll dup it out and you'll see better performance. Sometimes you'll dup it out, you'll see worse performance. If you see worse performance, obviously just kick the original back on and then revert back to option A um in order to get just another scaling campaign that you add to the pile.
[52:45]
The again, the biggest liability when you do option B is that you're just not going to see the same performance that you just did in the testing campaign when you officially pull it over to the winning campaign. That's the only thing I don't like about option B.
[53:00]
Option A will lead you to have like a fuckload of campaigns eventually that you're just like managing and maintaining. We have a great training inside of our inner circle weekly group call library on I think it's like how to spend 10k a day or how to spend 15k a day and I talk about this concept in it called foundational campaigns.
[53:20]
So it's a okay eventually to have like mini campaigns that are all like scaled up to their ceiling. You're just monitoring them for fatigue, overcoming fatigue when it inevitably occurs and just keeping them running and then you're constantly looking for new foundational campaigns to add to the pile.
[53:38]
This is like the extended training of that is how you could look at this specific conversation we're having today.
[53:45]
CHRIS: And each dupe allows us for the opportunity to scale at that point, right? We don't have to stick to the previous budget was time.
[53:52]
Both of these specific options that we look at here, these these both lead to scaling.
[53:58]
CHRIS: That's that's the ultimate goal of both option A and B is to scale.
[54:02]
CHRIS: Heard.
[54:05]
Perfect sense. Appreciate you.
[54:08]
Uh Tyler says, "How do you determine if it's an ad problem or a landing page problem for a non-performance? What if it's the same ads were to perform with a different page?"
[54:20]
Yeah. So Tyler, long story short, that's bottleneck analysis. I mean, to be clear, some ads might be good, but yes, to your point, like they have incongruency to the landing page. So if they did have a better landing page, could it theoretically perform better? Arguably yes.
[54:38]
Are you actually ever going to take the time to do that? Probably fucking not. So, I wouldn't think with that personally. I would just again isolate my constants and variables. I'm not looking to add landing pages as a variable when I'm focused on creative testing. I'm going to leave my landing page as a constant in that example.
[54:58]
So, I'm going to again I'm going to blame the creative more than I'm going to blame the landing page in that example.
[55:08]
Timothy says, "How critical is it to have a majority hook campaign versus a bunch of mini foundational campaigns?"
[55:15]
Wrong question. Highly irrelevant. Doesn't mean anything. I don't give a fuck about that. I'm going to have many campaigns that are all going to be in theory scaling campaigns. Some of them might have majority hooks. Some of them might have tinier hooks that have less total spend.
[55:32]
I don't give a fuck I'm scaling them all up to their ceiling and try to make as much money as possible. That's what actually matters in that example. There's no other way to judge it.
[55:45]
Like I don't give a fuck if I have like a majority of my campaigns as majority hook campaigns or if I have a majority of my campaigns as tiny hook campaigns and I'm chipping out smaller little pocket audiences in the market doesn't mean anything. It's a it's a highly technical irrelevant question.
[56:02] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Creative Surplus: Efficient Production Strategy"
NÃšMEROS: 20 ads filmados, 10 hooks, 10 bodies, 10 CTAs
VISUAL: Estrutura de produÃ§Ã£o de vÃ­deo dividida em trÃªs seÃ§Ãµes: Hook, Body, CTA
[56:05]
Okay, I'm going to answer some of these other things um that I know you guys need to know that you did not ask. So allow me to continue here. So when you look at getting to the point where you have a creative surplus, okay, you have to be able to do it in an efficient way.
[56:22]
If I logically need to test a hundred creatives at a time, like how can I efficiently go about doing that? Okay, so typically when we look at campaigns, okay, there's really three components to an ad that we want to be thinking with. We have the hook, we have the body, and we have the CTA. Okay, pretty simple.
[56:42]
So, what I typically like to do is I like to have clients film in a way where these are sectioned out. Okay, so I'll have them film like let's use the example of 20 total ads. Okay, but I'll make sure when they film the ads that the three sections of the ad that they're filming are separated into this where they're going to film, you know, a hook, a body, and a CTA.
[57:08]
They're going to film it all the way through. They're not even going to recognize what they're doing. But what I'm going to do with it, let's just say I get 10 ads altogether. So let's say that I have a client that just films 10 total video ads.
[57:22]
Well, in that case, I should have 10 different hooks. I should have 10 different bodies and I should have 10 pretty similar CTAs.
[57:32] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Priority Hierarchy in Ad Components"
NÃšMEROS: NÃ£o especificados
VISUAL: PirÃ¢mide de prioridade: Hook (topo - mais importante), Body (meio), CTA (base - 20Âº lugar em importÃ¢ncia)
[57:35]
I want to be very very fair in saying that the main thing that matters more than anything else is the hooks. Okay? In terms of priority, it's like this matters most. This m this matters second most. This honestly matters like fucking 20th, you know, like it's like very low on my priority list of giving a fuck about the uh the CTAs.
[57:58]
It just gives me an opportunity to swap in variables. And then what I'm then going to train my editing team to do, so it's like in this example, I have an entire ad. Okay, so let's say I have ad one. Okay, ad one is again broken up into three pot three spots. It has the hook, it has the body, and it has the CTA.
[58:20]
Okay, the client just filmed the ad all the way through. But what my editing team is trained to do is they're trained to clip the hook and to clip the body. And also, of course, they're going to clip the CPA, the CTA.
[58:35]
But again, I can't stress it enough. The fucking CTAs are like so they're viewed so little, it doesn't even really matter. These these are the variables that matter most are these two variables getting clipped out.
[58:50] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Creative Combination Math: 10 Ads = 1,000 Variations"
NÃšMEROS: 10 hooks x 10 bodies x 10 CTAs = 1,000 combinaÃ§Ãµes totais possÃ­veis
VISUAL: FÃ³rmula matemÃ¡tica visual, diagrama de combinaÃ§Ãµes mostrando multiplicaÃ§Ã£o exponencial
[58:52]
Okay? Because again, if I have 10 total hooks, 10 total bodies, and 10 total CTAs, what I then get to do is I get to mix all that shit together. So, in this example down here, right, I'm going to have add, okay? And same logic. I have my hook, I have my body, and I have my CTA.
[59:12]
Okay? So, now in this example, watch what I can do, right? Even just with two ads, it's like I can have the hook from the first ad, the body from the second ad, and the CTA from the second ad. Okay. And then on the next one, I can have the hook from two, the body from one, and the CTA from one.
[59:32]
Okay? And then in another, like I said, at this point, you could create more variations by swapping in the actual hook itself, right? So it's like now I got hook one, body one, CTA 2. Okay? And then blah blah blah blah blah vice versa.
[59:48]
So with the original just two ads that I got, I can interchange four extra, right? technically technically six total combinations, but realist realistically four extra because all I'm really going to do is just vary the hooks and the body.
[1:00:05]
The CTAs have such little effectiveness, but like technically it counts as a new ad. So, it's good to still follow swapping out the CTAs at the end because in its entirety, it's going to benefit us when we're just trying to overcome ad fatigue with essentially the exact same creative that historically worked, but like it counts as a new creative. You know what I mean?
[1:00:28]
So, it's like we're not really going to get a lot of creative testing out of it, but we're going to be able to throw it into our ad fatigue protocol if and when that comes up.
[1:00:38]
So if you do there's this there's this math problem that you probably all were taught in school about how okay you got like three pairs of shoes you know two pairs of pants two shorts five shirts you know like how many total outfits do you got?
[1:00:52]
So what you end up getting in terms of total creatives here like first of all you get 10 create you got 10 hooks 10 bodies that's technically 100 different combinations in itself and then you also have 10 CTAs right so in this example technically 10 time 10 time 10 you got a thousand total variations off of filming just 10 different ads in this example assuming that you actually followed through with 10 hooks 10 different bodies and 10 total CTAs.
[1:01:22] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "B-Roll Multiplication Strategy"
NÃšMEROS: NÃ£o especificados
VISUAL: Pasta de B-roll aprovado, setas mostrando como B-roll multiplica ainda mais as variaÃ§Ãµes de criativos
[1:01:25]
And so then your editing team, what you also need to be able to have outside of this is ideally an approved B-roll folder. Okay? Because then what you can also do with all of these creatives that you've got up here, like I can add B-roll into these as well.
[1:01:42]
And that extends the total quantity of creatives that I'm going to get to be able to test without having to actually go and film a hundred individual creators. Okay. So now it's like I got my original ad and then I got the ad that's got the B-roll in it and then I got ad two and then I got ad two with B-roll in it. So I just multiplied those right there.
[1:02:02]
And need I not say with an approved B-roll folder, I can also just swap out the B-roll in it to different B-roll and now it counts as a totally new ad again. You understand?
[1:02:12]
So, it's very important to note that when you're creative testing, typically the part that has the most impact is the hook and the body. Okay? Adding B-roll, changing B-roll, removing B-roll generally doesn't have huge implications on the performance of a of an ad.
[1:02:32]
However, it's like it can and you know, to be fair, sometimes it does. So, we still want to test against it and see how it performs overall. But, I can't stress it enough. main thing that we actually give a fuck about here is getting as many hooks and bodies filmed that we can. Okay, that's what matters most.
[1:02:52]
And obviously, as you can see, I get a lot of different creatives from this as a result of this strategy, which makes it really easy for me to end up hitting my numbers of having a 100 creatives and a giant creative surplus.
[1:03:05]
Okay, those two things are super easy to accomplish once we get to the point where we master this specific system of just creative in general.
[1:03:15] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Ad Scripting Examples"
NÃšMEROS: 10-20 ads filmados por sessÃ£o tÃ­pica
VISUAL: Screenshots de documentos de roteiros de ads, estrutura Hook-Body-CTA claramente marcada
[1:03:18]
Okay. So, when I typically go film or I have clients film, um what I'll do is I'll type up ad ideas for them and you know, as an example, like I literally just had some, uh some ad ideas that we typed up here yesterday.
[1:03:35]
Um, sometimes I'll make it super clear and then other times I won't. Like let's use this one as an example. Like this was this was for Russ recently. Um, I have the hook, the body, and the CTA. So like sometimes I'll label them like this.
[1:03:52]
And then other times it's like I don't label them and I'll just I'll throw the client things like these. But this is typically how it looks. Hooks, body, CTA. And I try to vary these, you know.
[1:04:05]
So like as an example, this is Andrew. Andrew's got, you know, and all these are different, right? So it's like look how different the hooks are. You see that? Like none of the hooks are the same. Like all the hooks open completely differently.
[1:04:22]
And it's the same thing with the body. Like the bodies of the ads are also all relevant to the topic of those specific ads, right? And this makes it super easy to have that interchangeable effect that we're looking for.
[1:04:35]
And in most instances, I'll send clients anywhere from like as low as 10 if they're like super fucking busy and rich. And in other instances, like, you know, this one I just sent for a client literally yesterday.
[1:04:50]
Um, this is an example where I don't have it broken out into hook, body, CTA. These ones are just shorter in general. So, these ones are like little 60 seconds, but they all still have different opening hooks that the client's going to say. The body is just shorter.
[1:05:08]
And then I end it with this case with some 90 second longer ads. And so this gives me a lot of room to play. So in this case I have 10 ads that are 60 seconds and I have 10 ads that are 90 seconds that are a little longer and more expanded.
[1:05:25]
So I get I get a lot of and just keep in mind like in the example I provided to you in the math, I had the client shooting 10 ads. It's like in almost all instances I'm having clients shoot far more than 10 ads because these people generally don't spend a lot of time actually filming shit you know?
[1:05:45] ğŸ“Š SLIDE DETECTADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEXTO: "Real-World Filming Example: Cody Sanchez"
NÃšMEROS: NÃ£o especificados
VISUAL: CenÃ¡rio de filmagem em evento, estratÃ©gia de captura de conteÃºdo em momentos oportunos
[1:05:48]
It's like I might get them for like dude, if I get Cody Sanchez I I'm actually going and talking to at a at a Cody Sanchez event right after this that she's doing here in Miami. And literally I'm bringing my video guy to the fucking venue and I'm going to have Cody film some ads while we're there because that's going to give me an opportunity to get more B-roll, more creatives, but she's also not just setting aside the time to consistently film at this volume.
[1:06:18]
She only sets aside so much time to film in general. And so a lot of her film times goes towards her organic stuff and very little time goes towards me on the ad side. Even though in this case she makes a fuckload of money from her ads.
[1:06:32]
Most of my clients to be fair like they'll do what I actually need them to do and they'll film at a minimum of like 10 to 20 total ads at a time whenever I actually need them to go and film.
[1:06:45]
So it's important to note like it's very contingent upon who you're working with or you know whether you as a business in this case actually have the time to justify like actually filming this shit It doesn't take a lot of time to be clear. Um it's not tough.
[1:07:02]
I will throw um a few of these ad idea documents that I create uh into the chat after this. And if you so happen to be watching the recording, then down in the description of the video, there's like a little download section and you'll see a few of those ad ideas there that you can download and look through.
[1:07:22]
You can throw them into chat GPT or whatever in terms of formatting or whatever LLM you'd like.
ğŸ“Š RESUMO EXECUTIVO
NÃšMEROS PRINCIPAIS:
$35K/dia - nÃ­vel de gasto de Jacob (case study de alto volume)
$2K/dia - mÃ©dia de gasto da maioria dos participantes
70% vÃ­deos / 30% imagens - ratio recomendado
6.6% hit rate - taxa de sucesso de Jacob em testes criativos
Sub 10% hit rate - taxa tÃ­pica da maioria dos anunciantes
100 criativos testados = 6-7 vencedores (com 6.6% hit rate)
200 criativos testados = 12-14 vencedores
25 ads por ad set - estrutura tÃ­pica com $2K/dia
4 ad sets - configuraÃ§Ã£o padrÃ£o para distribuiÃ§Ã£o igual
$500/dia por ad set - divisÃ£o de orÃ§amento
$20/dia por ad (teÃ³rico) - gasto por criativo individual
10 hooks x 10 bodies x 10 CTAs = 1,000 variaÃ§Ãµes possÃ­veis
10-20 ads - volume mÃ­nimo de filmagem por sessÃ£o
40-60% engagement rate - benchmark histÃ³rico de VSSL
85% play rate / 6% engagement - exemplo de VSSL com baixo desempenho
$1,000-1,500 - gasto tÃ­pico antes de avaliar testes (2-3 dias)
Alguns milhares de pessoas alcanÃ§adas - mÃ­nimo para determinaÃ§Ã£o de vencedor/perdedor
60 segundos - duraÃ§Ã£o de ads curtos
90 segundos - duraÃ§Ã£o de ads expandidos
ESTRATÃ‰GIAS E SISTEMAS:
Framework de Constantes vs VariÃ¡veis
Maioria de elementos como constantes
Isolar uma variÃ¡vel especÃ­fica para teste
Componentes de teste criativo: body copy, vÃ­deos/imagens, headlines
Espectro de Interesse de Leads
Curious â†’ General Interest â†’ Convicted
VÃ­deos geram leads de maior intenÃ§Ã£o
Imagens geram mais alcance mas menor intenÃ§Ã£o
Creative Surplus (Excedente Criativo)
Produzir "far more than needed"
Postura proativa vs reativa
Permite resposta rÃ¡pida Ã  fadiga de ads
PrincÃ­pio de Desapego
"I don't want to be right, I just want to make money"
Evitar viÃ©s pessoal em testes
Aceitar baixa taxa de sucesso (hit rate)
Estrutura de Campanha de Teste
ABO (Ad Set Budget Optimization) vs CBO
MÃºltiplos ad sets com mesma configuraÃ§Ã£o
25 ads por ad set (exemplo com 100 ads totais)
Individual ads vs Dynamic creative
EstratÃ©gia de DistribuiÃ§Ã£o
Buscar alcance igual inicial
Quebrar em mais ad sets se nÃ£o houver distribuiÃ§Ã£o
Budget maior = menos ad sets necessÃ¡rios
Budget menor = mais ad sets necessÃ¡rios
Budget vs Tempo
Budget alto = teste mais rÃ¡pido
Budget baixo = teste mais longo
Aguardar milhares de pessoas alcanÃ§adas antes de conclusÃµes
Majority Hooks vs Tiny Hooks
Majority hooks: ampla distribuiÃ§Ã£o, menos fadiga, maior escala ($8-10K+/dia)
Tiny hooks: fadiga rÃ¡pida, distribuiÃ§Ã£o limitada, viram campanhas fundamentais
Bias AlgorÃ­tmico
Interpretar como positivo (indica majority hook)
NÃ£o culpar a plataforma, ajustar estrutura
Ads com menos alcance podem ser tiny hooks viÃ¡veis
Breakout Strategy (EstratÃ©gia de SeparaÃ§Ã£o)
Fase 1: Teste inicial com todos os criativos
Fase 2: Separar majority hooks em campanha prÃ³pria
Fase 3: Separar tiny hooks em campanha prÃ³pria
Testar cada grupo isoladamente
ConversÃ£o de Vencedores
OpÃ§Ã£o A: Converter campanha de teste em campanha de escala (maior probabilidade)
OpÃ§Ã£o B: Puxar vencedores para campanha existente (menor probabilidade)
Duplicar ad sets ao adicionar novos criativos
Campanhas Fundamentais
MÃºltiplas campanhas escaladas atÃ© o teto
Monitorar fadiga constantemente
Adicionar novas campanhas fundamentais ao "pile"
ProduÃ§Ã£o Eficiente de Criativos
Estrutura: Hook + Body + CTA
Prioridade: Hook (1Âº), Body (2Âº), CTA (20Âº)
EdiÃ§Ã£o em seÃ§Ãµes separadas para intercÃ¢mbio
MatemÃ¡tica de CombinaÃ§Ãµes
10 ads filmados = 1,000 variaÃ§Ãµes possÃ­veis
Intercambiar hooks, bodies e CTAs
Adicionar/remover/trocar B-roll para mais variaÃ§Ãµes
EstratÃ©gia de B-Roll
Pasta aprovada de B-roll
Multiplica variaÃ§Ãµes sem nova filmagem
Ajuda no combate Ã  fadiga
Volume de ProduÃ§Ã£o
MÃ­nimo 10-20 ads por sessÃ£o de filmagem
Aproveitar momentos oportunos (eventos, etc)
Variar hooks completamente entre ads
ESTRUTURA:
Hierarquia de Campanha
Campanha
â”œâ”€â”€ Ad Set 1 ($500/dia)
â”‚   â”œâ”€â”€ Ad 1-25
â”œâ”€â”€ Ad Set 2 ($500/dia)
â”‚   â”œâ”€â”€ Ad 1-25
â”œâ”€â”€ Ad Set 3 ($500/dia)
â”‚   â”œâ”€â”€ Ad 1-25
â””â”€â”€ Ad Set 4 ($500/dia)
   â”œâ”€â”€ Ad 1-25
Fluxo de Breakout
Teste Inicial (100 ads)
â†“
Identificar Majority Hooks (26 ads exemplo)
â†“
Reach Testing Campaign
â†“
Identificar Tiny Hooks (74 ads restantes)
â†“
Creative Testing Campaign
â†“
Converter Vencedores (OpÃ§Ã£o A ou B)
Componentes de Ad
Ad 1: Hook 1 + Body 1 + CTA 1
Ad 2: Hook 1 + Body 2 + CTA 2
Ad 3: Hook 2 + Body 1 + CTA 1
Ad 4: Hook 2 + Body 2 + CTA 2
[+ variaÃ§Ãµes com B-roll]
MÃ©tricas de AvaliaÃ§Ã£o (ordem de prioridade)
ROAS (ideal mas demora)
Cost Per Result
Link CTR
CPM
Thumb Stop Rate
DADOS FINANCEIROS:
$2,000/dia - orÃ§amento tÃ­pico de teste
$500/dia - por ad set (com 4 ad sets)
$20/dia - teÃ³rico por ad individual
$5-50/dia - variaÃ§Ã£o real de gasto por ad
$35,000/dia - nÃ­vel de escala alto (case Jacob)
$5,000/dia - orÃ§amento de teste para alto volume
$8,000-10,000+/dia - potencial de escala de majority hooks
$100-200/dia - orÃ§amento baixo (reduzir volume de testes)
$1,000-1,500 - gasto antes de avaliaÃ§Ã£o inicial (2-3 dias)
DURAÃ‡ÃƒO: ~1h07min FORMATO: Webinar/treinamento ao vivo do Inner Circle com Q&A interativo